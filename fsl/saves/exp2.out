Namespace(T_max=20, backbone='resnet50', base_optimizer='SGD', batch_size=2048, dampening=0.0, dataset='cifar100fs', dataset_root=None, first_augment='CropResize', head='SimpleMLP', image_channels=3, image_mean=(0.5071, 0.4867, 0.4408), image_size=32, image_std=(0.2675, 0.2565, 0.2761), jitter_strength=1.0, loss_function='NTXent', model='SimCLRModel', momentum=0.05, nesterov=True, ntxent_temp=1.0, num_epochs=200, num_workers=4, pin_memory=True, pretrained=False, progress=False, projection_dim=128, save_path='exp2.pth', scheduler='CosineAnnealingLR', second_augment='GaussBlur', secondary_optimizer='LARS', shuffle=True, simple_opt=True, weight_decay=1e-06)
SimCLRModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (head): SimpleMLP(
    (l1): Linear(in_features=1000, out_features=512, bias=True)
    (relu): ReLU(inplace=True)
    (l2): Linear(in_features=512, out_features=128, bias=True)
  )
)
Starting Training
-----------------
(    22.264s) Epoch 001/200: Loss=8.163225 (best so far)
(    20.655s) Epoch 002/200: Loss=7.936259 (best so far)
(    20.332s) Epoch 003/200: Loss=7.809529 (best so far)
(    20.582s) Epoch 004/200: Loss=7.761923 (best so far)
(    20.732s) Epoch 005/200: Loss=7.745928 (best so far)
(    20.777s) Epoch 006/200: Loss=7.730519 (best so far)
(    20.921s) Epoch 007/200: Loss=7.706717 (best so far)
(    20.958s) Epoch 008/200: Loss=7.697227 (best so far)
(    21.072s) Epoch 009/200: Loss=7.683933 (best so far)
(    21.156s) Epoch 010/200: Loss=7.680660 (best so far)
(    21.204s) Epoch 011/200: Loss=7.667492 (best so far)
(    21.263s) Epoch 012/200: Loss=7.661758 (best so far)
(    21.304s) Epoch 013/200: Loss=7.654919 (best so far)
(    21.302s) Epoch 014/200: Loss=7.647439 (best so far)
(    21.268s) Epoch 015/200: Loss=7.640490 (best so far)
(    21.329s) Epoch 016/200: Loss=7.635790 (best so far)
(    21.307s) Epoch 017/200: Loss=7.627095 (best so far)
(    21.397s) Epoch 018/200: Loss=7.624571 (best so far)
(    21.360s) Epoch 019/200: Loss=7.624879
(    21.335s) Epoch 020/200: Loss=7.619269 (best so far)
(    21.357s) Epoch 021/200: Loss=7.615584 (best so far)
(    21.352s) Epoch 022/200: Loss=7.612911 (best so far)
(    21.365s) Epoch 023/200: Loss=7.610341 (best so far)
(    21.361s) Epoch 024/200: Loss=7.607226 (best so far)
(    21.349s) Epoch 025/200: Loss=7.601410 (best so far)
(    21.378s) Epoch 026/200: Loss=7.600075 (best so far)
(    21.342s) Epoch 027/200: Loss=7.599519 (best so far)
(    21.388s) Epoch 028/200: Loss=7.591695 (best so far)
(    21.388s) Epoch 029/200: Loss=7.590933 (best so far)
(    21.315s) Epoch 030/200: Loss=7.590154 (best so far)
(    21.387s) Epoch 031/200: Loss=7.584195 (best so far)
(    21.380s) Epoch 032/200: Loss=7.581924 (best so far)
(    21.332s) Epoch 033/200: Loss=7.583341
(    21.382s) Epoch 034/200: Loss=7.577987 (best so far)
(    21.374s) Epoch 035/200: Loss=7.577925 (best so far)
(    21.362s) Epoch 036/200: Loss=7.576784 (best so far)
(    21.463s) Epoch 037/200: Loss=7.575516 (best so far)
(    21.364s) Epoch 038/200: Loss=7.574202 (best so far)
(    21.351s) Epoch 039/200: Loss=7.574212
(    21.367s) Epoch 040/200: Loss=7.573481 (best so far)
(    21.372s) Epoch 041/200: Loss=7.572917 (best so far)
(    21.308s) Epoch 042/200: Loss=7.571535 (best so far)
(    21.349s) Epoch 043/200: Loss=7.571313 (best so far)
(    21.376s) Epoch 044/200: Loss=7.570086 (best so far)
(    21.293s) Epoch 045/200: Loss=7.571114
(    21.365s) Epoch 046/200: Loss=7.567820 (best so far)
(    21.339s) Epoch 047/200: Loss=7.569127
(    21.313s) Epoch 048/200: Loss=7.567882
(    21.354s) Epoch 049/200: Loss=7.566471 (best so far)
(    21.368s) Epoch 050/200: Loss=7.565237 (best so far)
(    21.325s) Epoch 051/200: Loss=7.567179
(    21.368s) Epoch 052/200: Loss=7.566524
(    21.393s) Epoch 053/200: Loss=7.566395
(    21.328s) Epoch 054/200: Loss=7.565115 (best so far)
(    21.327s) Epoch 055/200: Loss=7.563111 (best so far)
(    21.339s) Epoch 056/200: Loss=7.561111 (best so far)
(    21.340s) Epoch 057/200: Loss=7.559147 (best so far)
(    21.351s) Epoch 058/200: Loss=7.560711
(    21.349s) Epoch 059/200: Loss=7.557073 (best so far)
(    21.727s) Epoch 060/200: Loss=7.556335 (best so far)
(    21.498s) Epoch 061/200: Loss=7.556520
(    22.090s) Epoch 062/200: Loss=7.555203 (best so far)
(    21.563s) Epoch 063/200: Loss=7.553171 (best so far)
(    21.474s) Epoch 064/200: Loss=7.552868 (best so far)
(    21.462s) Epoch 065/200: Loss=7.552615 (best so far)
(    21.474s) Epoch 066/200: Loss=7.552450 (best so far)
(    21.496s) Epoch 067/200: Loss=7.550853 (best so far)
(    21.530s) Epoch 068/200: Loss=7.549193 (best so far)
(    21.485s) Epoch 069/200: Loss=7.549630
(    21.489s) Epoch 070/200: Loss=7.547656 (best so far)
(    21.519s) Epoch 071/200: Loss=7.547196 (best so far)
(    21.515s) Epoch 072/200: Loss=7.547381
(    21.509s) Epoch 073/200: Loss=7.545764 (best so far)
(    21.498s) Epoch 074/200: Loss=7.546647
(    21.495s) Epoch 075/200: Loss=7.545667 (best so far)
(    21.462s) Epoch 076/200: Loss=7.545522 (best so far)
(    21.506s) Epoch 077/200: Loss=7.545594
(    21.513s) Epoch 078/200: Loss=7.543405 (best so far)
(    21.557s) Epoch 079/200: Loss=7.543544
(    21.584s) Epoch 080/200: Loss=7.542888 (best so far)
(    21.553s) Epoch 081/200: Loss=7.542020 (best so far)
(    21.566s) Epoch 082/200: Loss=7.542352
(    21.528s) Epoch 083/200: Loss=7.541108 (best so far)
(    21.600s) Epoch 084/200: Loss=7.540581 (best so far)
(    21.620s) Epoch 085/200: Loss=7.540362 (best so far)
(    21.624s) Epoch 086/200: Loss=7.539226 (best so far)
(    21.623s) Epoch 087/200: Loss=7.539309
(    21.565s) Epoch 088/200: Loss=7.539797
(    21.553s) Epoch 089/200: Loss=7.538216 (best so far)
(    21.546s) Epoch 090/200: Loss=7.537833 (best so far)
(    21.527s) Epoch 091/200: Loss=7.536998 (best so far)
(    21.548s) Epoch 092/200: Loss=7.537827
(    21.574s) Epoch 093/200: Loss=7.536578 (best so far)
(    21.502s) Epoch 094/200: Loss=7.536394 (best so far)
(    21.556s) Epoch 095/200: Loss=7.534002 (best so far)
(    21.546s) Epoch 096/200: Loss=7.535794
(    21.582s) Epoch 097/200: Loss=7.535548
(    21.556s) Epoch 098/200: Loss=7.534760
(    21.548s) Epoch 099/200: Loss=7.533516 (best so far)
(    21.447s) Epoch 100/200: Loss=7.533521
(    21.462s) Epoch 101/200: Loss=7.533005 (best so far)
(    21.427s) Epoch 102/200: Loss=7.533319
(    21.386s) Epoch 103/200: Loss=7.531945 (best so far)
(    21.424s) Epoch 104/200: Loss=7.532610
(    21.425s) Epoch 105/200: Loss=7.532617
(    21.398s) Epoch 106/200: Loss=7.531409 (best so far)
(    21.448s) Epoch 107/200: Loss=7.532650
(    21.502s) Epoch 108/200: Loss=7.531080 (best so far)
(    21.495s) Epoch 109/200: Loss=7.531026 (best so far)
(    21.497s) Epoch 110/200: Loss=7.530839 (best so far)
(    21.366s) Epoch 111/200: Loss=7.530879
(    21.533s) Epoch 112/200: Loss=7.530043 (best so far)
(    21.390s) Epoch 113/200: Loss=7.530817
(    21.423s) Epoch 114/200: Loss=7.528921 (best so far)
(    21.533s) Epoch 115/200: Loss=7.529631
(    21.491s) Epoch 116/200: Loss=7.529589
(    21.394s) Epoch 117/200: Loss=7.529769
(    21.371s) Epoch 118/200: Loss=7.527751 (best so far)
(    21.424s) Epoch 119/200: Loss=7.528109
(    21.377s) Epoch 120/200: Loss=7.528164
(    21.407s) Epoch 121/200: Loss=7.528294
(    21.402s) Epoch 122/200: Loss=7.527700 (best so far)
(    21.372s) Epoch 123/200: Loss=7.526787 (best so far)
(    21.391s) Epoch 124/200: Loss=7.526739 (best so far)
(    21.428s) Epoch 125/200: Loss=7.527105
(    21.462s) Epoch 126/200: Loss=7.526590 (best so far)
(    21.510s) Epoch 127/200: Loss=7.525866 (best so far)
(    21.450s) Epoch 128/200: Loss=7.526022
(    21.490s) Epoch 129/200: Loss=7.526139
(    21.436s) Epoch 130/200: Loss=7.524957 (best so far)
(    21.374s) Epoch 131/200: Loss=7.525725
(    21.366s) Epoch 132/200: Loss=7.525688
(    21.366s) Epoch 133/200: Loss=7.524076 (best so far)
(    21.414s) Epoch 134/200: Loss=7.526081
(    21.361s) Epoch 135/200: Loss=7.524744
(    21.356s) Epoch 136/200: Loss=7.525381
(    21.407s) Epoch 137/200: Loss=7.523873 (best so far)
(    21.351s) Epoch 138/200: Loss=7.524584
(    21.400s) Epoch 139/200: Loss=7.524289
(    21.405s) Epoch 140/200: Loss=7.524334
(    21.405s) Epoch 141/200: Loss=7.523816 (best so far)
(    21.355s) Epoch 142/200: Loss=7.523499 (best so far)
(    21.398s) Epoch 143/200: Loss=7.522274 (best so far)
(    21.392s) Epoch 144/200: Loss=7.523053
(    21.390s) Epoch 145/200: Loss=7.522942
(    21.425s) Epoch 146/200: Loss=7.522989
(    21.393s) Epoch 147/200: Loss=7.522525
(    21.383s) Epoch 148/200: Loss=7.522435
(    21.410s) Epoch 149/200: Loss=7.523292
(    21.402s) Epoch 150/200: Loss=7.522675
(    21.446s) Epoch 151/200: Loss=7.522467
(    21.462s) Epoch 152/200: Loss=7.521949 (best so far)
(    21.502s) Epoch 153/200: Loss=7.521518 (best so far)
(    21.381s) Epoch 154/200: Loss=7.520908 (best so far)
(    21.452s) Epoch 155/200: Loss=7.521087
(    21.461s) Epoch 156/200: Loss=7.521814
(    21.423s) Epoch 157/200: Loss=7.520703 (best so far)
(    21.410s) Epoch 158/200: Loss=7.521491
(    21.497s) Epoch 159/200: Loss=7.520980
(    21.446s) Epoch 160/200: Loss=7.520866
(    21.434s) Epoch 161/200: Loss=7.520112 (best so far)
(    21.504s) Epoch 162/200: Loss=7.520922
(    21.504s) Epoch 163/200: Loss=7.520102 (best so far)
(    21.485s) Epoch 164/200: Loss=7.520295
(    21.519s) Epoch 165/200: Loss=7.519748 (best so far)
(    21.353s) Epoch 166/200: Loss=7.519755
(    21.430s) Epoch 167/200: Loss=7.519790
(    21.391s) Epoch 168/200: Loss=7.519354 (best so far)
(    21.382s) Epoch 169/200: Loss=7.519198 (best so far)
(    21.447s) Epoch 170/200: Loss=7.519765
(    21.362s) Epoch 171/200: Loss=7.520247
(    21.362s) Epoch 172/200: Loss=7.519529
(    21.493s) Epoch 173/200: Loss=7.518560 (best so far)
(    21.418s) Epoch 174/200: Loss=7.518401 (best so far)
(    21.400s) Epoch 175/200: Loss=7.518845
(    21.426s) Epoch 176/200: Loss=7.518333 (best so far)
(    21.390s) Epoch 177/200: Loss=7.519147
(    21.397s) Epoch 178/200: Loss=7.518155 (best so far)
(    21.391s) Epoch 179/200: Loss=7.518375
(    21.428s) Epoch 180/200: Loss=7.518768
(    21.406s) Epoch 181/200: Loss=7.517707 (best so far)
(    21.393s) Epoch 182/200: Loss=7.517822
(    21.423s) Epoch 183/200: Loss=7.517836
(    21.356s) Epoch 184/200: Loss=7.517238 (best so far)
(    21.403s) Epoch 185/200: Loss=7.518257
(    21.418s) Epoch 186/200: Loss=7.517401
(    21.386s) Epoch 187/200: Loss=7.517890
(    21.406s) Epoch 188/200: Loss=7.517778
(    21.402s) Epoch 189/200: Loss=7.517215 (best so far)
(    21.395s) Epoch 190/200: Loss=7.517493
(    21.430s) Epoch 191/200: Loss=7.517005 (best so far)
(    21.446s) Epoch 192/200: Loss=7.516447 (best so far)
(    21.396s) Epoch 193/200: Loss=7.516237 (best so far)
(    21.468s) Epoch 194/200: Loss=7.516550
(    21.511s) Epoch 195/200: Loss=7.516997
(    21.416s) Epoch 196/200: Loss=7.516404
(    21.403s) Epoch 197/200: Loss=7.516667
(    21.556s) Epoch 198/200: Loss=7.515837 (best so far)
(    21.519s) Epoch 199/200: Loss=7.516288
(    21.408s) Epoch 200/200: Loss=7.515880
Training for 200 epochs took 4281.443s total and 21.407s average
Saving model and options to exp2.pth
done!!!
