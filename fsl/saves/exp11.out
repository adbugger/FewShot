Namespace(T_max=200, backbone='resnet50', base_optimizer='SGD', batch_size=4096, dampening=0.0, dataset='cifar100fs', dataset_root=None, first_augment='CropResize', head='SimpleMLP', image_channels=3, image_mean=(0.5071, 0.4867, 0.4408), image_size=32, image_std=(0.2675, 0.2565, 0.2761), jitter_strength=1.0, loss_function='NTXent', model='SimCLRModel', momentum=0.05, nesterov=True, ntxent_temp=1.0, num_epochs=200, num_workers=4, pin_memory=True, pretrained=False, progress=False, projection_dim=128, save_path='saves/exp11.pth', scheduler='CosineAnnealingLR', second_augment='GaussBlur', secondary_optimizer='LARS', shuffle=True, simple_opt=False, weight_decay=1e-06)
DataParallel(
  (module): SimCLRModel(
    (backbone): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=1000, bias=True)
    )
    (head): SimpleMLP(
      (l1): Linear(in_features=1000, out_features=512, bias=True)
      (relu): ReLU(inplace=True)
      (l2): Linear(in_features=512, out_features=128, bias=True)
    )
  )
)
Starting Training
-----------------
(  39.138s) Epoch 001/200: Loss=8.594733 (best so far)
(  13.727s) Epoch 002/200: Loss=8.394019 (best so far)
(  10.820s) Epoch 003/200: Loss=8.298897 (best so far)
(  10.875s) Epoch 004/200: Loss=8.267290 (best so far)
(  10.834s) Epoch 005/200: Loss=8.245193 (best so far)
(  10.853s) Epoch 006/200: Loss=8.230436 (best so far)
(  10.781s) Epoch 007/200: Loss=8.217821 (best so far)
(  10.821s) Epoch 008/200: Loss=8.211399 (best so far)
(  10.807s) Epoch 009/200: Loss=8.202156 (best so far)
(  10.870s) Epoch 010/200: Loss=8.196708 (best so far)
(  10.775s) Epoch 011/200: Loss=8.192968 (best so far)
(  10.849s) Epoch 012/200: Loss=8.187535 (best so far)
(  10.801s) Epoch 013/200: Loss=8.185323 (best so far)
(  10.791s) Epoch 014/200: Loss=8.181572 (best so far)
(  10.794s) Epoch 015/200: Loss=8.178767 (best so far)
(  10.917s) Epoch 016/200: Loss=8.174696 (best so far)
(  10.863s) Epoch 017/200: Loss=8.173836 (best so far)
(  10.875s) Epoch 018/200: Loss=8.171140 (best so far)
(  10.878s) Epoch 019/200: Loss=8.168792 (best so far)
(  10.925s) Epoch 020/200: Loss=8.166259 (best so far)
(  10.877s) Epoch 021/200: Loss=8.164611 (best so far)
(  10.863s) Epoch 022/200: Loss=8.163148 (best so far)
(  10.940s) Epoch 023/200: Loss=8.161125 (best so far)
(  10.845s) Epoch 024/200: Loss=8.159858 (best so far)
(  10.803s) Epoch 025/200: Loss=8.158275 (best so far)
(  10.888s) Epoch 026/200: Loss=8.156632 (best so far)
(  10.851s) Epoch 027/200: Loss=8.155998 (best so far)
(  10.806s) Epoch 028/200: Loss=8.154529 (best so far)
(  10.830s) Epoch 029/200: Loss=8.152797 (best so far)
(  10.859s) Epoch 030/200: Loss=8.151750 (best so far)
(  10.885s) Epoch 031/200: Loss=8.150670 (best so far)
(  10.896s) Epoch 032/200: Loss=8.149120 (best so far)
(  10.822s) Epoch 033/200: Loss=8.147528 (best so far)
(  10.840s) Epoch 034/200: Loss=8.147576
(  10.895s) Epoch 035/200: Loss=8.146712 (best so far)
(  10.844s) Epoch 036/200: Loss=8.145738 (best so far)
(  10.856s) Epoch 037/200: Loss=8.145330 (best so far)
(  10.904s) Epoch 038/200: Loss=8.143651 (best so far)
(  10.885s) Epoch 039/200: Loss=8.143823
(  10.879s) Epoch 040/200: Loss=8.143245 (best so far)
(  10.838s) Epoch 041/200: Loss=8.142342 (best so far)
(  10.855s) Epoch 042/200: Loss=8.141402 (best so far)
(  10.827s) Epoch 043/200: Loss=8.140700 (best so far)
(  10.882s) Epoch 044/200: Loss=8.139861 (best so far)
(  10.881s) Epoch 045/200: Loss=8.139732 (best so far)
(  10.866s) Epoch 046/200: Loss=8.139563 (best so far)
(  10.821s) Epoch 047/200: Loss=8.138800 (best so far)
(  10.839s) Epoch 048/200: Loss=8.137614 (best so far)
(  10.878s) Epoch 049/200: Loss=8.137840
(  10.880s) Epoch 050/200: Loss=8.136835 (best so far)
(  10.879s) Epoch 051/200: Loss=8.136543 (best so far)
(  10.887s) Epoch 052/200: Loss=8.135699 (best so far)
(  10.855s) Epoch 053/200: Loss=8.136471
(  10.825s) Epoch 054/200: Loss=8.136339
(  10.858s) Epoch 055/200: Loss=8.134232 (best so far)
(  10.899s) Epoch 056/200: Loss=8.134547
(  10.861s) Epoch 057/200: Loss=8.134045 (best so far)
(  10.865s) Epoch 058/200: Loss=8.133772 (best so far)
(  10.914s) Epoch 059/200: Loss=8.133449 (best so far)
(  10.851s) Epoch 060/200: Loss=8.132817 (best so far)
(  10.917s) Epoch 061/200: Loss=8.132623 (best so far)
(  10.900s) Epoch 062/200: Loss=8.132364 (best so far)
(  10.900s) Epoch 063/200: Loss=8.131936 (best so far)
(  10.872s) Epoch 064/200: Loss=8.131444 (best so far)
(  10.868s) Epoch 065/200: Loss=8.131451
(  10.840s) Epoch 066/200: Loss=8.131729
(  10.837s) Epoch 067/200: Loss=8.130553 (best so far)
(  10.888s) Epoch 068/200: Loss=8.130202 (best so far)
(  10.859s) Epoch 069/200: Loss=8.130400
(  10.824s) Epoch 070/200: Loss=8.129381 (best so far)
(  10.885s) Epoch 071/200: Loss=8.128871 (best so far)
(  10.876s) Epoch 072/200: Loss=8.128952
(  10.883s) Epoch 073/200: Loss=8.128174 (best so far)
(  10.862s) Epoch 074/200: Loss=8.128786
(  10.847s) Epoch 075/200: Loss=8.128507
(  10.855s) Epoch 076/200: Loss=8.127917 (best so far)
(  10.911s) Epoch 077/200: Loss=8.127474 (best so far)
(  10.920s) Epoch 078/200: Loss=8.126544 (best so far)
(  10.920s) Epoch 079/200: Loss=8.127594
(  10.882s) Epoch 080/200: Loss=8.126991
(  10.907s) Epoch 081/200: Loss=8.126574
(  10.828s) Epoch 082/200: Loss=8.127002
(  10.824s) Epoch 083/200: Loss=8.126039 (best so far)
(  10.844s) Epoch 084/200: Loss=8.126035 (best so far)
(  10.823s) Epoch 085/200: Loss=8.126450
(  10.832s) Epoch 086/200: Loss=8.125224 (best so far)
(  10.897s) Epoch 087/200: Loss=8.125188 (best so far)
(  10.883s) Epoch 088/200: Loss=8.125501
(  10.895s) Epoch 089/200: Loss=8.125372
(  10.903s) Epoch 090/200: Loss=8.125063 (best so far)
(  10.848s) Epoch 091/200: Loss=8.124804 (best so far)
(  10.880s) Epoch 092/200: Loss=8.123990 (best so far)
(  10.855s) Epoch 093/200: Loss=8.123998
(  10.864s) Epoch 094/200: Loss=8.124393
(  10.932s) Epoch 095/200: Loss=8.123466 (best so far)
(  10.898s) Epoch 096/200: Loss=8.123540
(  10.900s) Epoch 097/200: Loss=8.123537
(  10.900s) Epoch 098/200: Loss=8.122898 (best so far)
(  10.906s) Epoch 099/200: Loss=8.122520 (best so far)
(  10.860s) Epoch 100/200: Loss=8.122888
(  10.826s) Epoch 101/200: Loss=8.122635
(  10.839s) Epoch 102/200: Loss=8.122470 (best so far)
(  10.879s) Epoch 103/200: Loss=8.122142 (best so far)
(  10.915s) Epoch 104/200: Loss=8.122210
(  10.890s) Epoch 105/200: Loss=8.121813 (best so far)
(  10.881s) Epoch 106/200: Loss=8.122140
(  10.879s) Epoch 107/200: Loss=8.121550 (best so far)
(  10.862s) Epoch 108/200: Loss=8.121109 (best so far)
(  10.907s) Epoch 109/200: Loss=8.120740 (best so far)
(  10.865s) Epoch 110/200: Loss=8.121054
(  10.878s) Epoch 111/200: Loss=8.121004
(  10.928s) Epoch 112/200: Loss=8.120623 (best so far)
(  10.798s) Epoch 113/200: Loss=8.121380
(  10.919s) Epoch 114/200: Loss=8.120499 (best so far)
(  10.919s) Epoch 115/200: Loss=8.120432 (best so far)
(  10.940s) Epoch 116/200: Loss=8.120410 (best so far)
(  10.914s) Epoch 117/200: Loss=8.120086 (best so far)
(  10.911s) Epoch 118/200: Loss=8.119914 (best so far)
(  10.875s) Epoch 119/200: Loss=8.119967
(  10.871s) Epoch 120/200: Loss=8.119422 (best so far)
(  10.812s) Epoch 121/200: Loss=8.119534
(  10.814s) Epoch 122/200: Loss=8.119003 (best so far)
(  10.870s) Epoch 123/200: Loss=8.119022
(  10.891s) Epoch 124/200: Loss=8.119159
(  10.898s) Epoch 125/200: Loss=8.118928 (best so far)
(  10.920s) Epoch 126/200: Loss=8.118990
(  10.897s) Epoch 127/200: Loss=8.118621 (best so far)
(  10.877s) Epoch 128/200: Loss=8.118253 (best so far)
(  10.898s) Epoch 129/200: Loss=8.118886
(  10.848s) Epoch 130/200: Loss=8.118449
(  10.857s) Epoch 131/200: Loss=8.118599
(  10.880s) Epoch 132/200: Loss=8.118470
(  10.925s) Epoch 133/200: Loss=8.117784 (best so far)
(  10.868s) Epoch 134/200: Loss=8.118534
(  10.905s) Epoch 135/200: Loss=8.118546
(  10.897s) Epoch 136/200: Loss=8.118544
(  10.912s) Epoch 137/200: Loss=8.117697 (best so far)
(  10.875s) Epoch 138/200: Loss=8.118168
(  10.841s) Epoch 139/200: Loss=8.117953
(  10.853s) Epoch 140/200: Loss=8.117447 (best so far)
(  10.847s) Epoch 141/200: Loss=8.117300 (best so far)
(  10.851s) Epoch 142/200: Loss=8.117782
(  10.836s) Epoch 143/200: Loss=8.117111 (best so far)
(  10.866s) Epoch 144/200: Loss=8.117579
(  10.782s) Epoch 145/200: Loss=8.117566
(  10.845s) Epoch 146/200: Loss=8.117213
(  10.872s) Epoch 147/200: Loss=8.117310
(  10.829s) Epoch 148/200: Loss=8.117227
(  10.887s) Epoch 149/200: Loss=8.117157
(  10.841s) Epoch 150/200: Loss=8.116570 (best so far)
(  10.880s) Epoch 151/200: Loss=8.117137
(  10.945s) Epoch 152/200: Loss=8.117045
(  10.894s) Epoch 153/200: Loss=8.117165
(  10.905s) Epoch 154/200: Loss=8.117255
(  10.910s) Epoch 155/200: Loss=8.116533 (best so far)
(  10.914s) Epoch 156/200: Loss=8.116613
(  10.880s) Epoch 157/200: Loss=8.116366 (best so far)
(  10.877s) Epoch 158/200: Loss=8.116579
(  10.861s) Epoch 159/200: Loss=8.116604
(  10.816s) Epoch 160/200: Loss=8.116336 (best so far)
(  10.801s) Epoch 161/200: Loss=8.116440
(  10.867s) Epoch 162/200: Loss=8.116522
(  10.907s) Epoch 163/200: Loss=8.116260 (best so far)
(  10.902s) Epoch 164/200: Loss=8.116271
(  10.870s) Epoch 165/200: Loss=8.116501
(  10.871s) Epoch 166/200: Loss=8.116634
(  10.884s) Epoch 167/200: Loss=8.116639
(  10.914s) Epoch 168/200: Loss=8.115585 (best so far)
(  10.869s) Epoch 169/200: Loss=8.116295
(  10.934s) Epoch 170/200: Loss=8.116541
(  10.918s) Epoch 171/200: Loss=8.116366
(  10.927s) Epoch 172/200: Loss=8.116482
(  10.899s) Epoch 173/200: Loss=8.115874
(  10.861s) Epoch 174/200: Loss=8.115950
(  10.857s) Epoch 175/200: Loss=8.115793
(  10.870s) Epoch 176/200: Loss=8.115882
(  10.859s) Epoch 177/200: Loss=8.116503
(  10.896s) Epoch 178/200: Loss=8.115900
(  10.876s) Epoch 179/200: Loss=8.115409 (best so far)
(  10.869s) Epoch 180/200: Loss=8.116056
(  10.836s) Epoch 181/200: Loss=8.115680
(  10.839s) Epoch 182/200: Loss=8.115502
(  10.871s) Epoch 183/200: Loss=8.115920
(  10.792s) Epoch 184/200: Loss=8.115462
(  10.827s) Epoch 185/200: Loss=8.116070
(  10.905s) Epoch 186/200: Loss=8.115275 (best so far)
(  10.930s) Epoch 187/200: Loss=8.115734
(  10.956s) Epoch 188/200: Loss=8.116421
(  10.894s) Epoch 189/200: Loss=8.115769
(  10.879s) Epoch 190/200: Loss=8.115746
(  10.897s) Epoch 191/200: Loss=8.115887
(  10.890s) Epoch 192/200: Loss=8.115361
(  10.889s) Epoch 193/200: Loss=8.115845
(  10.861s) Epoch 194/200: Loss=8.115694
(  10.876s) Epoch 195/200: Loss=8.115666
(  10.890s) Epoch 196/200: Loss=8.115772
(  10.886s) Epoch 197/200: Loss=8.116289
(  10.882s) Epoch 198/200: Loss=8.115394
(  10.862s) Epoch 199/200: Loss=8.115603
(  10.841s) Epoch 200/200: Loss=8.115155 (best so far)
Training for 200 epochs took 2205.274s total and 11.026s average
Saving best model and options to saves/exp11.pth
