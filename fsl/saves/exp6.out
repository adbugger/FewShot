Namespace(T_max=20, backbone='resnet50', base_optimizer='SGD', batch_size=2048, dampening=0.0, dataset='cifar100fs', dataset_root=None, first_augment='CropResize', head='SimpleMLP', image_channels=3, image_mean=(0.5071, 0.4867, 0.4408), image_size=32, image_std=(0.2675, 0.2565, 0.2761), jitter_strength=1.0, loss_function='NTXent', model='SimCLRModel', momentum=0.01, nesterov=True, ntxent_temp=1.0, num_epochs=200, num_workers=4, pin_memory=True, pretrained=False, progress=False, projection_dim=128, save_path='exp6.pth', scheduler='CosineAnnealingLR', second_augment='GaussBlur', secondary_optimizer='LARS', shuffle=True, simple_opt=True, weight_decay=1e-06)
SimCLRModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (head): SimpleMLP(
    (l1): Linear(in_features=1000, out_features=512, bias=True)
    (relu): ReLU(inplace=True)
    (l2): Linear(in_features=512, out_features=128, bias=True)
  )
)
Starting Training
-----------------
(    20.196s) Epoch 001/200: Loss=8.161278 (best so far)
(    20.253s) Epoch 002/200: Loss=7.989066 (best so far)
(    20.344s) Epoch 003/200: Loss=7.813363 (best so far)
(    20.669s) Epoch 004/200: Loss=7.762818 (best so far)
(    20.644s) Epoch 005/200: Loss=7.753451 (best so far)
(    21.276s) Epoch 006/200: Loss=7.746133 (best so far)
(    20.748s) Epoch 007/200: Loss=7.740086 (best so far)
(    21.307s) Epoch 008/200: Loss=7.730828 (best so far)
(    20.744s) Epoch 009/200: Loss=7.717645 (best so far)
(    21.391s) Epoch 010/200: Loss=7.707452 (best so far)
(    20.800s) Epoch 011/200: Loss=7.700066 (best so far)
(    20.852s) Epoch 012/200: Loss=7.697967 (best so far)
(    21.480s) Epoch 013/200: Loss=7.694296 (best so far)
(    20.971s) Epoch 014/200: Loss=7.689397 (best so far)
(    21.440s) Epoch 015/200: Loss=7.691817
(    20.817s) Epoch 016/200: Loss=7.701699
(    20.846s) Epoch 017/200: Loss=7.696547
(    21.343s) Epoch 018/200: Loss=7.696952
(    20.864s) Epoch 019/200: Loss=7.684080 (best so far)
(    21.419s) Epoch 020/200: Loss=7.686289
(    20.917s) Epoch 021/200: Loss=7.672202 (best so far)
(    21.475s) Epoch 022/200: Loss=7.659178 (best so far)
(    21.340s) Epoch 023/200: Loss=7.656498 (best so far)
(    20.924s) Epoch 024/200: Loss=7.652525 (best so far)
(    21.452s) Epoch 025/200: Loss=7.652476 (best so far)
(    21.245s) Epoch 026/200: Loss=7.648874 (best so far)
(    21.149s) Epoch 027/200: Loss=7.647688 (best so far)
(    20.724s) Epoch 028/200: Loss=7.641178 (best so far)
(    20.769s) Epoch 029/200: Loss=7.636212 (best so far)
(    20.708s) Epoch 030/200: Loss=7.626475 (best so far)
(    20.778s) Epoch 031/200: Loss=7.624398 (best so far)
(    20.825s) Epoch 032/200: Loss=7.620762 (best so far)
(    20.712s) Epoch 033/200: Loss=7.617257 (best so far)
(    20.584s) Epoch 034/200: Loss=7.614232 (best so far)
(    21.168s) Epoch 035/200: Loss=7.613391 (best so far)
(    20.831s) Epoch 036/200: Loss=7.612099 (best so far)
(    20.896s) Epoch 037/200: Loss=7.609453 (best so far)
(    21.137s) Epoch 038/200: Loss=7.606379 (best so far)
(    21.155s) Epoch 039/200: Loss=7.604043 (best so far)
(    20.913s) Epoch 040/200: Loss=7.601778 (best so far)
(    20.871s) Epoch 041/200: Loss=7.600412 (best so far)
(    20.770s) Epoch 042/200: Loss=7.598150 (best so far)
(    20.840s) Epoch 043/200: Loss=7.593545 (best so far)
(    20.546s) Epoch 044/200: Loss=7.593669
(    21.075s) Epoch 045/200: Loss=7.592577 (best so far)
(    20.557s) Epoch 046/200: Loss=7.589588 (best so far)
(    20.886s) Epoch 047/200: Loss=7.589816
(    20.762s) Epoch 048/200: Loss=7.587673 (best so far)
(    21.344s) Epoch 049/200: Loss=7.584583 (best so far)
(    20.759s) Epoch 050/200: Loss=7.579577 (best so far)
(    20.732s) Epoch 051/200: Loss=7.579144 (best so far)
(    21.383s) Epoch 052/200: Loss=7.576268 (best so far)
(    20.755s) Epoch 053/200: Loss=7.578091
(    21.242s) Epoch 054/200: Loss=7.577641
(    20.877s) Epoch 055/200: Loss=7.577725
(    21.118s) Epoch 056/200: Loss=7.572837 (best so far)
(    20.771s) Epoch 057/200: Loss=7.570431 (best so far)
(    21.357s) Epoch 058/200: Loss=7.571688
(    20.757s) Epoch 059/200: Loss=7.571369
(    21.332s) Epoch 060/200: Loss=7.570027 (best so far)
(    20.960s) Epoch 061/200: Loss=7.569638 (best so far)
(    21.294s) Epoch 062/200: Loss=7.570979
(    20.718s) Epoch 063/200: Loss=7.569285 (best so far)
(    21.400s) Epoch 064/200: Loss=7.568750 (best so far)
(    20.700s) Epoch 065/200: Loss=7.567290 (best so far)
(    21.156s) Epoch 066/200: Loss=7.567986
(    20.926s) Epoch 067/200: Loss=7.567821
(    21.247s) Epoch 068/200: Loss=7.566576 (best so far)
(    20.785s) Epoch 069/200: Loss=7.567189
(    21.426s) Epoch 070/200: Loss=7.566250 (best so far)
(    20.827s) Epoch 071/200: Loss=7.566081 (best so far)
(    20.894s) Epoch 072/200: Loss=7.566866
(    20.687s) Epoch 073/200: Loss=7.564770 (best so far)
(    20.731s) Epoch 074/200: Loss=7.566137
(    20.781s) Epoch 075/200: Loss=7.566891
(    21.371s) Epoch 076/200: Loss=7.566196
(    20.783s) Epoch 077/200: Loss=7.565979
(    20.860s) Epoch 078/200: Loss=7.564864
(    20.756s) Epoch 079/200: Loss=7.566033
(    20.873s) Epoch 080/200: Loss=7.564923
(    20.672s) Epoch 081/200: Loss=7.564973
(    20.628s) Epoch 082/200: Loss=7.564855
(    21.151s) Epoch 083/200: Loss=7.564783
(    20.909s) Epoch 084/200: Loss=7.563624 (best so far)
(    20.783s) Epoch 085/200: Loss=7.563976
(    20.865s) Epoch 086/200: Loss=7.564062
(    21.177s) Epoch 087/200: Loss=7.563876
(    20.801s) Epoch 088/200: Loss=7.563337 (best so far)
(    21.120s) Epoch 089/200: Loss=7.563731
(    20.888s) Epoch 090/200: Loss=7.562112 (best so far)
(    20.708s) Epoch 091/200: Loss=7.561825 (best so far)
(    21.028s) Epoch 092/200: Loss=7.563121
(    20.633s) Epoch 093/200: Loss=7.561239 (best so far)
(    20.873s) Epoch 094/200: Loss=7.560173 (best so far)
(    20.700s) Epoch 095/200: Loss=7.559597 (best so far)
(    20.749s) Epoch 096/200: Loss=7.559795
(    21.381s) Epoch 097/200: Loss=7.560902
(    20.773s) Epoch 098/200: Loss=7.559737
(    20.800s) Epoch 099/200: Loss=7.558930 (best so far)
(    21.111s) Epoch 100/200: Loss=7.557018 (best so far)
(    20.770s) Epoch 101/200: Loss=7.556245 (best so far)
(    20.797s) Epoch 102/200: Loss=7.557201
(    21.204s) Epoch 103/200: Loss=7.554422 (best so far)
(    21.262s) Epoch 104/200: Loss=7.556260
(    20.738s) Epoch 105/200: Loss=7.555673
(    21.396s) Epoch 106/200: Loss=7.555594
(    20.770s) Epoch 107/200: Loss=7.553941 (best so far)
(    20.745s) Epoch 108/200: Loss=7.554888
(    20.895s) Epoch 109/200: Loss=7.552133 (best so far)
(    20.779s) Epoch 110/200: Loss=7.551944 (best so far)
(    20.980s) Epoch 111/200: Loss=7.553255
(    20.785s) Epoch 112/200: Loss=7.550778 (best so far)
(    20.702s) Epoch 113/200: Loss=7.553793
(    21.217s) Epoch 114/200: Loss=7.549725 (best so far)
(    21.405s) Epoch 115/200: Loss=7.550723
(    20.794s) Epoch 116/200: Loss=7.549943
(    21.418s) Epoch 117/200: Loss=7.548297 (best so far)
(    20.778s) Epoch 118/200: Loss=7.548497
(    20.751s) Epoch 119/200: Loss=7.547701 (best so far)
(    20.748s) Epoch 120/200: Loss=7.548287
(    20.821s) Epoch 121/200: Loss=7.547033 (best so far)
(    20.776s) Epoch 122/200: Loss=7.546551 (best so far)
(    20.751s) Epoch 123/200: Loss=7.545071 (best so far)
(    20.719s) Epoch 124/200: Loss=7.546325
(    20.727s) Epoch 125/200: Loss=7.545093
(    21.890s) Epoch 126/200: Loss=7.543850 (best so far)
(    25.141s) Epoch 127/200: Loss=7.544515
(    20.869s) Epoch 128/200: Loss=7.543728 (best so far)
(    21.061s) Epoch 129/200: Loss=7.543948
(    21.004s) Epoch 130/200: Loss=7.542329 (best so far)
(    21.110s) Epoch 131/200: Loss=7.544228
(    21.045s) Epoch 132/200: Loss=7.544085
(    20.710s) Epoch 133/200: Loss=7.541034 (best so far)
(    21.084s) Epoch 134/200: Loss=7.541582
(    21.064s) Epoch 135/200: Loss=7.541684
(    20.787s) Epoch 136/200: Loss=7.540512 (best so far)
(    20.564s) Epoch 137/200: Loss=7.540657
(    20.708s) Epoch 138/200: Loss=7.541154
(    20.547s) Epoch 139/200: Loss=7.541393
(    20.721s) Epoch 140/200: Loss=7.540642
(    20.536s) Epoch 141/200: Loss=7.539459 (best so far)
(    21.139s) Epoch 142/200: Loss=7.539790
(    21.095s) Epoch 143/200: Loss=7.538407 (best so far)
(    20.893s) Epoch 144/200: Loss=7.539647
(    20.910s) Epoch 145/200: Loss=7.539760
(    21.114s) Epoch 146/200: Loss=7.538712
(    20.741s) Epoch 147/200: Loss=7.538411
(    21.081s) Epoch 148/200: Loss=7.538278 (best so far)
(    20.832s) Epoch 149/200: Loss=7.538661
(    20.715s) Epoch 150/200: Loss=7.538583
(    21.093s) Epoch 151/200: Loss=7.537299 (best so far)
(    20.710s) Epoch 152/200: Loss=7.538372
(    20.697s) Epoch 153/200: Loss=7.538822
(    20.844s) Epoch 154/200: Loss=7.536567 (best so far)
(    20.842s) Epoch 155/200: Loss=7.536829
(    20.716s) Epoch 156/200: Loss=7.536845
(    20.882s) Epoch 157/200: Loss=7.535391 (best so far)
(    20.731s) Epoch 158/200: Loss=7.535788
(    20.877s) Epoch 159/200: Loss=7.536974
(    20.707s) Epoch 160/200: Loss=7.535603
(    20.691s) Epoch 161/200: Loss=7.535434
(    20.770s) Epoch 162/200: Loss=7.535454
(    20.993s) Epoch 163/200: Loss=7.535318 (best so far)
(    20.697s) Epoch 164/200: Loss=7.534357 (best so far)
(    20.913s) Epoch 165/200: Loss=7.534250 (best so far)
(    20.822s) Epoch 166/200: Loss=7.533735 (best so far)
(    20.843s) Epoch 167/200: Loss=7.533826
(    21.027s) Epoch 168/200: Loss=7.534729
(    20.726s) Epoch 169/200: Loss=7.533543 (best so far)
(    20.755s) Epoch 170/200: Loss=7.534237
(    20.660s) Epoch 171/200: Loss=7.534398
(    20.670s) Epoch 172/200: Loss=7.532952 (best so far)
(    20.683s) Epoch 173/200: Loss=7.532714 (best so far)
(    20.828s) Epoch 174/200: Loss=7.532272 (best so far)
(    20.722s) Epoch 175/200: Loss=7.532667
(    20.883s) Epoch 176/200: Loss=7.532406
(    20.675s) Epoch 177/200: Loss=7.532805
(    20.553s) Epoch 178/200: Loss=7.531980 (best so far)
(    20.824s) Epoch 179/200: Loss=7.531230 (best so far)
(    20.846s) Epoch 180/200: Loss=7.532128
(    20.881s) Epoch 181/200: Loss=7.530537 (best so far)
(    20.698s) Epoch 182/200: Loss=7.532215
(    20.878s) Epoch 183/200: Loss=7.531216
(    20.941s) Epoch 184/200: Loss=7.530592
(    20.834s) Epoch 185/200: Loss=7.531009
(    20.875s) Epoch 186/200: Loss=7.530036 (best so far)
(    20.663s) Epoch 187/200: Loss=7.530524
(    20.677s) Epoch 188/200: Loss=7.530086
(    20.965s) Epoch 189/200: Loss=7.529326 (best so far)
(    20.674s) Epoch 190/200: Loss=7.529303 (best so far)
(    20.679s) Epoch 191/200: Loss=7.528675 (best so far)
(    20.704s) Epoch 192/200: Loss=7.528464 (best so far)
(    20.848s) Epoch 193/200: Loss=7.527900 (best so far)
(    20.698s) Epoch 194/200: Loss=7.528880
(    21.042s) Epoch 195/200: Loss=7.528539
(    20.692s) Epoch 196/200: Loss=7.527576 (best so far)
(    20.874s) Epoch 197/200: Loss=7.528213
(    20.857s) Epoch 198/200: Loss=7.526691 (best so far)
(    20.847s) Epoch 199/200: Loss=7.527330
(    20.840s) Epoch 200/200: Loss=7.526701
Training for 200 epochs took 4183.703s total and 20.919s average
Saving model and options to exp6.pth
done!!!
