Namespace(T_max=20, backbone='resnet50', base_optimizer='SGD', batch_size=2048, dataset='cifar100fs', dataset_root=None, first_augment='CropResize', head='SimpleMLP', image_channels=3, image_mean=(0.5071, 0.4867, 0.4408), image_size=32, image_std=(0.2675, 0.2565, 0.2761), jitter_strength=1.0, loss_function='NTXent', model='SimCLRModel', ntxent_temp=1.0, num_epochs=200, num_workers=4, pin_memory=True, pretrained=False, progress=False, projection_dim=128, save_path='exp1.pth', scheduler='CosineAnnealingLR', second_augment='GaussBlur', secondary_optimizer='LARS', shuffle=True)
SimCLRModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (head): SimpleMLP(
    (l1): Linear(in_features=1000, out_features=512, bias=True)
    (relu): ReLU(inplace=True)
    (l2): Linear(in_features=512, out_features=128, bias=True)
  )
)
Starting Training
-----------------
(    22.632s) Epoch 001/200: Loss=7.929747
(    20.588s) Epoch 002/200: Loss=7.792133
(    20.378s) Epoch 003/200: Loss=7.696349
(    20.652s) Epoch 004/200: Loss=7.650393
(    20.835s) Epoch 005/200: Loss=7.626172
(    20.960s) Epoch 006/200: Loss=7.607178
(    21.041s) Epoch 007/200: Loss=7.593505
(    21.119s) Epoch 008/200: Loss=7.581767
(    21.183s) Epoch 009/200: Loss=7.573830
(    21.240s) Epoch 010/200: Loss=7.569006
(    21.288s) Epoch 011/200: Loss=7.565436
(    21.312s) Epoch 012/200: Loss=7.561642
(    21.347s) Epoch 013/200: Loss=7.558782
(    21.410s) Epoch 014/200: Loss=7.556920
(    21.345s) Epoch 015/200: Loss=7.555711
(    21.410s) Epoch 016/200: Loss=7.554408
(    21.477s) Epoch 017/200: Loss=7.554228
(    21.421s) Epoch 018/200: Loss=7.553365
(    21.400s) Epoch 019/200: Loss=7.552895
(    21.462s) Epoch 020/200: Loss=7.552830
(    21.444s) Epoch 021/200: Loss=7.552498
(    21.397s) Epoch 022/200: Loss=7.552994
(    21.480s) Epoch 023/200: Loss=7.553997
(    21.408s) Epoch 024/200: Loss=7.552722
(    21.424s) Epoch 025/200: Loss=7.552516
(    21.454s) Epoch 026/200: Loss=7.552402
(    21.414s) Epoch 027/200: Loss=7.553022
(    21.466s) Epoch 028/200: Loss=7.552186
(    21.422s) Epoch 029/200: Loss=7.552457
(    21.470s) Epoch 030/200: Loss=7.553010
(    21.481s) Epoch 031/200: Loss=7.552769
(    21.450s) Epoch 032/200: Loss=7.552594
(    21.404s) Epoch 033/200: Loss=7.552967
(    21.428s) Epoch 034/200: Loss=7.552629
(    21.448s) Epoch 035/200: Loss=7.552769
(    21.456s) Epoch 036/200: Loss=7.552391
(    21.453s) Epoch 037/200: Loss=7.551601
(    21.437s) Epoch 038/200: Loss=7.550793
(    21.448s) Epoch 039/200: Loss=7.549673
(    21.405s) Epoch 040/200: Loss=7.548630
(    21.459s) Epoch 041/200: Loss=7.547424
(    21.415s) Epoch 042/200: Loss=7.545405
(    21.407s) Epoch 043/200: Loss=7.544151
(    21.437s) Epoch 044/200: Loss=7.541864
(    21.393s) Epoch 045/200: Loss=7.541075
(    21.409s) Epoch 046/200: Loss=7.538559
(    21.466s) Epoch 047/200: Loss=7.538552
(    21.400s) Epoch 048/200: Loss=7.535830
(    21.434s) Epoch 049/200: Loss=7.534208
(    21.488s) Epoch 050/200: Loss=7.532980
(    21.431s) Epoch 051/200: Loss=7.532654
(    21.448s) Epoch 052/200: Loss=7.531030
(    21.440s) Epoch 053/200: Loss=7.531090
(    21.424s) Epoch 054/200: Loss=7.530038
(    21.421s) Epoch 055/200: Loss=7.528460
(    21.412s) Epoch 056/200: Loss=7.528530
(    21.461s) Epoch 057/200: Loss=7.527825
(    21.457s) Epoch 058/200: Loss=7.527589
(    21.417s) Epoch 059/200: Loss=7.527769
(    21.416s) Epoch 060/200: Loss=7.527732
(    21.458s) Epoch 061/200: Loss=7.528080
(    21.432s) Epoch 062/200: Loss=7.528254
(    21.478s) Epoch 063/200: Loss=7.527970
(    21.437s) Epoch 064/200: Loss=7.528188
(    21.440s) Epoch 065/200: Loss=7.527774
(    21.463s) Epoch 066/200: Loss=7.528085
(    21.440s) Epoch 067/200: Loss=7.527398
(    21.426s) Epoch 068/200: Loss=7.527770
(    21.439s) Epoch 069/200: Loss=7.528529
(    21.453s) Epoch 070/200: Loss=7.528002
(    21.472s) Epoch 071/200: Loss=7.528280
(    21.407s) Epoch 072/200: Loss=7.528631
(    21.440s) Epoch 073/200: Loss=7.528354
(    21.466s) Epoch 074/200: Loss=7.529809
(    21.431s) Epoch 075/200: Loss=7.530148
(    21.400s) Epoch 076/200: Loss=7.530236
(    21.462s) Epoch 077/200: Loss=7.530136
(    21.413s) Epoch 078/200: Loss=7.529513
(    21.421s) Epoch 079/200: Loss=7.529915
(    21.429s) Epoch 080/200: Loss=7.529383
(    21.450s) Epoch 081/200: Loss=7.528990
(    21.434s) Epoch 082/200: Loss=7.528894
(    21.414s) Epoch 083/200: Loss=7.527968
(    21.498s) Epoch 084/200: Loss=7.527379
(    21.396s) Epoch 085/200: Loss=7.526749
(    21.424s) Epoch 086/200: Loss=7.525412
(    21.473s) Epoch 087/200: Loss=7.524908
(    21.425s) Epoch 088/200: Loss=7.523986
(    21.428s) Epoch 089/200: Loss=7.522619
(    21.471s) Epoch 090/200: Loss=7.522546
(    21.394s) Epoch 091/200: Loss=7.521568
(    21.449s) Epoch 092/200: Loss=7.521250
(    21.506s) Epoch 093/200: Loss=7.519924
(    21.408s) Epoch 094/200: Loss=7.519895
(    21.426s) Epoch 095/200: Loss=7.518809
(    21.448s) Epoch 096/200: Loss=7.519184
(    21.437s) Epoch 097/200: Loss=7.519149
(    21.458s) Epoch 098/200: Loss=7.518961
(    21.438s) Epoch 099/200: Loss=7.518186
(    21.473s) Epoch 100/200: Loss=7.518506
(    21.414s) Epoch 101/200: Loss=7.518291
(    21.476s) Epoch 102/200: Loss=7.518694
(    21.426s) Epoch 103/200: Loss=7.518120
(    21.460s) Epoch 104/200: Loss=7.518311
(    21.414s) Epoch 105/200: Loss=7.518575
(    21.426s) Epoch 106/200: Loss=7.518476
(    21.439s) Epoch 107/200: Loss=7.518987
(    21.407s) Epoch 108/200: Loss=7.518495
(    21.399s) Epoch 109/200: Loss=7.518763
(    21.413s) Epoch 110/200: Loss=7.519307
(    21.404s) Epoch 111/200: Loss=7.519850
(    21.445s) Epoch 112/200: Loss=7.519333
(    21.405s) Epoch 113/200: Loss=7.520783
(    21.440s) Epoch 114/200: Loss=7.520009
(    21.423s) Epoch 115/200: Loss=7.520984
(    21.432s) Epoch 116/200: Loss=7.521135
(    21.419s) Epoch 117/200: Loss=7.521940
(    21.444s) Epoch 118/200: Loss=7.521020
(    21.477s) Epoch 119/200: Loss=7.521393
(    21.398s) Epoch 120/200: Loss=7.521516
(    21.402s) Epoch 121/200: Loss=7.521585
(    21.437s) Epoch 122/200: Loss=7.520752
(    21.393s) Epoch 123/200: Loss=7.520240
(    21.392s) Epoch 124/200: Loss=7.519474
(    21.438s) Epoch 125/200: Loss=7.519645
(    21.446s) Epoch 126/200: Loss=7.518339
(    21.410s) Epoch 127/200: Loss=7.518038
(    21.394s) Epoch 128/200: Loss=7.516754
(    21.410s) Epoch 129/200: Loss=7.517251
(    21.400s) Epoch 130/200: Loss=7.515814
(    21.462s) Epoch 131/200: Loss=7.515800
(    21.421s) Epoch 132/200: Loss=7.515570
(    21.418s) Epoch 133/200: Loss=7.513996
(    21.467s) Epoch 134/200: Loss=7.514371
(    21.397s) Epoch 135/200: Loss=7.514064
(    21.410s) Epoch 136/200: Loss=7.513865
(    21.512s) Epoch 137/200: Loss=7.513272
(    21.407s) Epoch 138/200: Loss=7.513889
(    21.466s) Epoch 139/200: Loss=7.513580
(    21.428s) Epoch 140/200: Loss=7.513745
(    21.419s) Epoch 141/200: Loss=7.513055
(    21.447s) Epoch 142/200: Loss=7.513479
(    21.447s) Epoch 143/200: Loss=7.512925
(    21.421s) Epoch 144/200: Loss=7.513602
(    21.424s) Epoch 145/200: Loss=7.513504
(    21.477s) Epoch 146/200: Loss=7.513385
(    21.412s) Epoch 147/200: Loss=7.513276
(    21.428s) Epoch 148/200: Loss=7.513669
(    21.411s) Epoch 149/200: Loss=7.514129
(    21.426s) Epoch 150/200: Loss=7.514503
(    21.432s) Epoch 151/200: Loss=7.514609
(    21.430s) Epoch 152/200: Loss=7.514901
(    21.432s) Epoch 153/200: Loss=7.514892
(    21.440s) Epoch 154/200: Loss=7.515208
(    21.433s) Epoch 155/200: Loss=7.515416
(    21.446s) Epoch 156/200: Loss=7.516346
(    21.396s) Epoch 157/200: Loss=7.515876
(    21.404s) Epoch 158/200: Loss=7.516802
(    21.441s) Epoch 159/200: Loss=7.516686
(    21.431s) Epoch 160/200: Loss=7.516463
(    21.423s) Epoch 161/200: Loss=7.516312
(    21.397s) Epoch 162/200: Loss=7.516701
(    21.447s) Epoch 163/200: Loss=7.515493
(    21.423s) Epoch 164/200: Loss=7.515713
(    21.403s) Epoch 165/200: Loss=7.515046
(    21.428s) Epoch 166/200: Loss=7.514590
(    21.438s) Epoch 167/200: Loss=7.514442
(    21.435s) Epoch 168/200: Loss=7.513323
(    21.373s) Epoch 169/200: Loss=7.512765
(    21.425s) Epoch 170/200: Loss=7.512829
(    21.382s) Epoch 171/200: Loss=7.512945
(    21.424s) Epoch 172/200: Loss=7.511809
(    21.437s) Epoch 173/200: Loss=7.510709
(    21.413s) Epoch 174/200: Loss=7.510448
(    21.451s) Epoch 175/200: Loss=7.510608
(    21.453s) Epoch 176/200: Loss=7.510082
(    21.447s) Epoch 177/200: Loss=7.510446
(    21.418s) Epoch 178/200: Loss=7.510043
(    21.446s) Epoch 179/200: Loss=7.509842
(    21.454s) Epoch 180/200: Loss=7.510932
(    21.475s) Epoch 181/200: Loss=7.509605
(    21.475s) Epoch 182/200: Loss=7.509887
(    21.443s) Epoch 183/200: Loss=7.509963
(    21.445s) Epoch 184/200: Loss=7.509945
(    21.437s) Epoch 185/200: Loss=7.510492
(    21.405s) Epoch 186/200: Loss=7.509850
(    21.420s) Epoch 187/200: Loss=7.510282
(    21.457s) Epoch 188/200: Loss=7.510210
(    21.415s) Epoch 189/200: Loss=7.510055
(    21.412s) Epoch 190/200: Loss=7.510605
(    21.470s) Epoch 191/200: Loss=7.510861
(    21.454s) Epoch 192/200: Loss=7.510861
(    21.420s) Epoch 193/200: Loss=7.510980
(    21.418s) Epoch 194/200: Loss=7.511845
(    21.456s) Epoch 195/200: Loss=7.512778
(    21.480s) Epoch 196/200: Loss=7.512702
(    21.455s) Epoch 197/200: Loss=7.513314
(    21.409s) Epoch 198/200: Loss=7.512965
(    21.462s) Epoch 199/200: Loss=7.513256
(    21.426s) Epoch 200/200: Loss=7.513000
Training for 200 epochs took 4282.597s total and 21.413s average
Saving model and options to exp1.pth
done!!!
