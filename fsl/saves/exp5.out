Namespace(T_max=20, backbone='resnet50', base_optimizer='SGD', batch_size=2048, dampening=0.0, dataset='cifar100fs', dataset_root=None, first_augment='CropResize', head='SimpleMLP', image_channels=3, image_mean=(0.5071, 0.4867, 0.4408), image_size=32, image_std=(0.2675, 0.2565, 0.2761), jitter_strength=1.0, loss_function='NTXent', model='SimCLRModel', momentum=0.001, nesterov=True, ntxent_temp=1.0, num_epochs=200, num_workers=4, pin_memory=True, pretrained=False, progress=False, projection_dim=128, save_path='exp5.pth', scheduler='CosineAnnealingLR', second_augment='GaussBlur', secondary_optimizer='LARS', shuffle=True, simple_opt=True, weight_decay=1e-06)
SimCLRModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (head): SimpleMLP(
    (l1): Linear(in_features=1000, out_features=512, bias=True)
    (relu): ReLU(inplace=True)
    (l2): Linear(in_features=512, out_features=128, bias=True)
  )
)
Starting Training
-----------------
(    22.377s) Epoch 001/200: Loss=8.152860 (best so far)
(    20.855s) Epoch 002/200: Loss=7.947262 (best so far)
(    20.552s) Epoch 003/200: Loss=7.817788 (best so far)
(    20.723s) Epoch 004/200: Loss=7.765446 (best so far)
(    20.887s) Epoch 005/200: Loss=7.747965 (best so far)
(    20.956s) Epoch 006/200: Loss=7.732350 (best so far)
(    21.055s) Epoch 007/200: Loss=7.713021 (best so far)
(    21.004s) Epoch 008/200: Loss=7.705145 (best so far)
(    21.016s) Epoch 009/200: Loss=7.694446 (best so far)
(    21.061s) Epoch 010/200: Loss=7.690020 (best so far)
(    21.063s) Epoch 011/200: Loss=7.675618 (best so far)
(    21.132s) Epoch 012/200: Loss=7.669482 (best so far)
(    21.059s) Epoch 013/200: Loss=7.665355 (best so far)
(    21.177s) Epoch 014/200: Loss=7.658755 (best so far)
(    21.087s) Epoch 015/200: Loss=7.655563 (best so far)
(    21.155s) Epoch 016/200: Loss=7.653648 (best so far)
(    21.168s) Epoch 017/200: Loss=7.653802
(    21.220s) Epoch 018/200: Loss=7.641477 (best so far)
(    21.223s) Epoch 019/200: Loss=7.637633 (best so far)
(    21.261s) Epoch 020/200: Loss=7.630415 (best so far)
(    21.215s) Epoch 021/200: Loss=7.624264 (best so far)
(    21.198s) Epoch 022/200: Loss=7.620593 (best so far)
(    21.211s) Epoch 023/200: Loss=7.615083 (best so far)
(    21.217s) Epoch 024/200: Loss=7.607259 (best so far)
(    21.213s) Epoch 025/200: Loss=7.605224 (best so far)
(    21.214s) Epoch 026/200: Loss=7.600817 (best so far)
(    21.277s) Epoch 027/200: Loss=7.599450 (best so far)
(    21.197s) Epoch 028/200: Loss=7.596365 (best so far)
(    21.172s) Epoch 029/200: Loss=7.591919 (best so far)
(    21.245s) Epoch 030/200: Loss=7.591549 (best so far)
(    21.236s) Epoch 031/200: Loss=7.586686 (best so far)
(    21.189s) Epoch 032/200: Loss=7.587021
(    21.231s) Epoch 033/200: Loss=7.584391 (best so far)
(    21.224s) Epoch 034/200: Loss=7.580933 (best so far)
(    21.188s) Epoch 035/200: Loss=7.580202 (best so far)
(    21.247s) Epoch 036/200: Loss=7.574841 (best so far)
(    21.220s) Epoch 037/200: Loss=7.576048
(    21.225s) Epoch 038/200: Loss=7.574124 (best so far)
(    21.211s) Epoch 039/200: Loss=7.572463 (best so far)
(    21.220s) Epoch 040/200: Loss=7.570292 (best so far)
(    21.226s) Epoch 041/200: Loss=7.571160
(    21.223s) Epoch 042/200: Loss=7.569647 (best so far)
(    21.225s) Epoch 043/200: Loss=7.566839 (best so far)
(    21.299s) Epoch 044/200: Loss=7.568417
(    21.258s) Epoch 045/200: Loss=7.567186
(    21.290s) Epoch 046/200: Loss=7.565475 (best so far)
(    21.200s) Epoch 047/200: Loss=7.563977 (best so far)
(    21.249s) Epoch 048/200: Loss=7.563918 (best so far)
(    21.210s) Epoch 049/200: Loss=7.561183 (best so far)
(    21.214s) Epoch 050/200: Loss=7.560832 (best so far)
(    21.192s) Epoch 051/200: Loss=7.560443 (best so far)
(    21.221s) Epoch 052/200: Loss=7.558894 (best so far)
(    21.212s) Epoch 053/200: Loss=7.560118
(    21.201s) Epoch 054/200: Loss=7.559865
(    21.242s) Epoch 055/200: Loss=7.556755 (best so far)
(    21.201s) Epoch 056/200: Loss=7.555257 (best so far)
(    21.355s) Epoch 057/200: Loss=7.555136 (best so far)
(    21.213s) Epoch 058/200: Loss=7.553460 (best so far)
(    21.170s) Epoch 059/200: Loss=7.553764
(    21.308s) Epoch 060/200: Loss=7.553542
(    21.227s) Epoch 061/200: Loss=7.552623 (best so far)
(    21.239s) Epoch 062/200: Loss=7.551518 (best so far)
(    21.208s) Epoch 063/200: Loss=7.550009 (best so far)
(    21.242s) Epoch 064/200: Loss=7.551449
(    21.260s) Epoch 065/200: Loss=7.548588 (best so far)
(    21.246s) Epoch 066/200: Loss=7.549189
(    21.196s) Epoch 067/200: Loss=7.546997 (best so far)
(    21.302s) Epoch 068/200: Loss=7.548245
(    21.219s) Epoch 069/200: Loss=7.546915 (best so far)
(    21.234s) Epoch 070/200: Loss=7.545848 (best so far)
(    21.208s) Epoch 071/200: Loss=7.545856
(    21.229s) Epoch 072/200: Loss=7.544936 (best so far)
(    21.246s) Epoch 073/200: Loss=7.543314 (best so far)
(    21.288s) Epoch 074/200: Loss=7.544662
(    21.277s) Epoch 075/200: Loss=7.544273
(    21.198s) Epoch 076/200: Loss=7.543101 (best so far)
(    21.215s) Epoch 077/200: Loss=7.542071 (best so far)
(    21.199s) Epoch 078/200: Loss=7.541454 (best so far)
(    21.184s) Epoch 079/200: Loss=7.541802
(    21.240s) Epoch 080/200: Loss=7.541411 (best so far)
(    21.178s) Epoch 081/200: Loss=7.540493 (best so far)
(    21.238s) Epoch 082/200: Loss=7.540995
(    21.173s) Epoch 083/200: Loss=7.539811 (best so far)
(    21.306s) Epoch 084/200: Loss=7.538951 (best so far)
(    21.162s) Epoch 085/200: Loss=7.539616
(    21.203s) Epoch 086/200: Loss=7.538375 (best so far)
(    21.209s) Epoch 087/200: Loss=7.537917 (best so far)
(    21.229s) Epoch 088/200: Loss=7.538536
(    21.287s) Epoch 089/200: Loss=7.537651 (best so far)
(    21.307s) Epoch 090/200: Loss=7.537051 (best so far)
(    21.160s) Epoch 091/200: Loss=7.537306
(    21.216s) Epoch 092/200: Loss=7.536187 (best so far)
(    21.263s) Epoch 093/200: Loss=7.535704 (best so far)
(    21.137s) Epoch 094/200: Loss=7.535545 (best so far)
(    21.187s) Epoch 095/200: Loss=7.535199 (best so far)
(    21.225s) Epoch 096/200: Loss=7.535279
(    21.220s) Epoch 097/200: Loss=7.535972
(    21.220s) Epoch 098/200: Loss=7.535204
(    21.245s) Epoch 099/200: Loss=7.533842 (best so far)
(    21.229s) Epoch 100/200: Loss=7.533737 (best so far)
(    21.244s) Epoch 101/200: Loss=7.533389 (best so far)
(    21.228s) Epoch 102/200: Loss=7.533768
(    21.194s) Epoch 103/200: Loss=7.532791 (best so far)
(    21.210s) Epoch 104/200: Loss=7.533054
(    21.218s) Epoch 105/200: Loss=7.532628 (best so far)
(    21.269s) Epoch 106/200: Loss=7.532377 (best so far)
(    21.249s) Epoch 107/200: Loss=7.532383
(    21.219s) Epoch 108/200: Loss=7.531685 (best so far)
(    21.205s) Epoch 109/200: Loss=7.531319 (best so far)
(    21.220s) Epoch 110/200: Loss=7.531870
(    21.180s) Epoch 111/200: Loss=7.531396
(    21.229s) Epoch 112/200: Loss=7.530730 (best so far)
(    21.211s) Epoch 113/200: Loss=7.531832
(    21.224s) Epoch 114/200: Loss=7.529794 (best so far)
(    21.267s) Epoch 115/200: Loss=7.530393
(    21.249s) Epoch 116/200: Loss=7.529534 (best so far)
(    21.201s) Epoch 117/200: Loss=7.530630
(    21.180s) Epoch 118/200: Loss=7.529453 (best so far)
(    21.207s) Epoch 119/200: Loss=7.528973 (best so far)
(    21.153s) Epoch 120/200: Loss=7.528758 (best so far)
(    21.213s) Epoch 121/200: Loss=7.529317
(    21.227s) Epoch 122/200: Loss=7.528749 (best so far)
(    21.210s) Epoch 123/200: Loss=7.528029 (best so far)
(    21.174s) Epoch 124/200: Loss=7.527899 (best so far)
(    21.243s) Epoch 125/200: Loss=7.527606 (best so far)
(    21.255s) Epoch 126/200: Loss=7.527448 (best so far)
(    21.216s) Epoch 127/200: Loss=7.527419 (best so far)
(    21.225s) Epoch 128/200: Loss=7.526594 (best so far)
(    21.236s) Epoch 129/200: Loss=7.527241
(    21.207s) Epoch 130/200: Loss=7.526243 (best so far)
(    21.181s) Epoch 131/200: Loss=7.526788
(    21.243s) Epoch 132/200: Loss=7.526366
(    21.188s) Epoch 133/200: Loss=7.525331 (best so far)
(    21.284s) Epoch 134/200: Loss=7.527199
(    21.225s) Epoch 135/200: Loss=7.526109
(    21.255s) Epoch 136/200: Loss=7.525804
(    21.222s) Epoch 137/200: Loss=7.525162 (best so far)
(    21.204s) Epoch 138/200: Loss=7.525954
(    21.256s) Epoch 139/200: Loss=7.525423
(    21.236s) Epoch 140/200: Loss=7.525553
(    21.251s) Epoch 141/200: Loss=7.524981 (best so far)
(    21.192s) Epoch 142/200: Loss=7.524468 (best so far)
(    21.229s) Epoch 143/200: Loss=7.524107 (best so far)
(    21.243s) Epoch 144/200: Loss=7.524850
(    21.236s) Epoch 145/200: Loss=7.524495
(    21.254s) Epoch 146/200: Loss=7.523323 (best so far)
(    21.177s) Epoch 147/200: Loss=7.523250 (best so far)
(    21.239s) Epoch 148/200: Loss=7.523417
(    21.238s) Epoch 149/200: Loss=7.524586
(    21.228s) Epoch 150/200: Loss=7.523519
(    21.233s) Epoch 151/200: Loss=7.523289
(    21.202s) Epoch 152/200: Loss=7.522994 (best so far)
(    21.226s) Epoch 153/200: Loss=7.522364 (best so far)
(    21.226s) Epoch 154/200: Loss=7.522671
(    21.248s) Epoch 155/200: Loss=7.522096 (best so far)
(    21.235s) Epoch 156/200: Loss=7.522829
(    21.212s) Epoch 157/200: Loss=7.521734 (best so far)
(    21.233s) Epoch 158/200: Loss=7.522329
(    21.199s) Epoch 159/200: Loss=7.521982
(    21.213s) Epoch 160/200: Loss=7.521766
(    21.299s) Epoch 161/200: Loss=7.521415 (best so far)
(    21.183s) Epoch 162/200: Loss=7.521390 (best so far)
(    21.240s) Epoch 163/200: Loss=7.520357 (best so far)
(    21.260s) Epoch 164/200: Loss=7.521461
(    21.234s) Epoch 165/200: Loss=7.520949
(    21.192s) Epoch 166/200: Loss=7.520614
(    21.238s) Epoch 167/200: Loss=7.520556
(    21.244s) Epoch 168/200: Loss=7.520607
(    21.224s) Epoch 169/200: Loss=7.519900 (best so far)
(    21.241s) Epoch 170/200: Loss=7.521101
(    21.235s) Epoch 171/200: Loss=7.521502
(    21.189s) Epoch 172/200: Loss=7.520368
(    21.224s) Epoch 173/200: Loss=7.519596 (best so far)
(    21.222s) Epoch 174/200: Loss=7.519601
(    21.199s) Epoch 175/200: Loss=7.519524 (best so far)
(    21.236s) Epoch 176/200: Loss=7.519237 (best so far)
(    21.244s) Epoch 177/200: Loss=7.520134
(    21.231s) Epoch 178/200: Loss=7.518981 (best so far)
(    21.256s) Epoch 179/200: Loss=7.519186
(    21.201s) Epoch 180/200: Loss=7.519880
(    21.239s) Epoch 181/200: Loss=7.518407 (best so far)
(    21.248s) Epoch 182/200: Loss=7.518611
(    21.243s) Epoch 183/200: Loss=7.518922
(    21.232s) Epoch 184/200: Loss=7.518399 (best so far)
(    21.217s) Epoch 185/200: Loss=7.519144
(    21.210s) Epoch 186/200: Loss=7.518418
(    21.232s) Epoch 187/200: Loss=7.518492
(    21.198s) Epoch 188/200: Loss=7.518215 (best so far)
(    21.234s) Epoch 189/200: Loss=7.517762 (best so far)
(    21.213s) Epoch 190/200: Loss=7.518114
(    21.218s) Epoch 191/200: Loss=7.517533 (best so far)
(    21.236s) Epoch 192/200: Loss=7.517290 (best so far)
(    21.254s) Epoch 193/200: Loss=7.517282 (best so far)
(    21.235s) Epoch 194/200: Loss=7.517078 (best so far)
(    21.251s) Epoch 195/200: Loss=7.517629
(    21.231s) Epoch 196/200: Loss=7.517324
(    21.181s) Epoch 197/200: Loss=7.517434
(    21.187s) Epoch 198/200: Loss=7.516585 (best so far)
(    21.252s) Epoch 199/200: Loss=7.517272
(    21.288s) Epoch 200/200: Loss=7.516788
Training for 200 epochs took 4242.699s total and 21.213s average
Saving model and options to exp5.pth
done!!!
