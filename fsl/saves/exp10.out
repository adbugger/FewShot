Namespace(T_max=200, backbone='resnet50', base_optimizer='SGD', batch_size=4096, dampening=0.0, dataset='cifar100fs', dataset_root=None, first_augment='CropResize', head='SimpleMLP', image_channels=3, image_mean=(0.5071, 0.4867, 0.4408), image_size=32, image_std=(0.2675, 0.2565, 0.2761), jitter_strength=1.0, loss_function='NTXent', model='SimCLRModel', momentum=0.05, nesterov=False, ntxent_temp=1.0, num_epochs=200, num_workers=4, pin_memory=True, pretrained=False, progress=False, projection_dim=128, save_path='exp10.pth', scheduler='CosineAnnealingLR', second_augment='GaussBlur', secondary_optimizer='LARS', shuffle=True, simple_opt=False, weight_decay=1e-06)
DataParallel(
  (module): SimCLRModel(
    (backbone): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=1000, bias=True)
    )
    (head): SimpleMLP(
      (l1): Linear(in_features=1000, out_features=512, bias=True)
      (relu): ReLU(inplace=True)
      (l2): Linear(in_features=512, out_features=128, bias=True)
    )
  )
)
Starting Training
-----------------
(  54.254s) Epoch 001/200: Loss=8.585695 (best so far)
(  15.894s) Epoch 002/200: Loss=8.394758 (best so far)
(  11.072s) Epoch 003/200: Loss=8.299534 (best so far)
(  11.123s) Epoch 004/200: Loss=8.265576 (best so far)
(  11.019s) Epoch 005/200: Loss=8.245863 (best so far)
(  11.046s) Epoch 006/200: Loss=8.229346 (best so far)
(  11.098s) Epoch 007/200: Loss=8.215800 (best so far)
(  11.141s) Epoch 008/200: Loss=8.209154 (best so far)
(  11.034s) Epoch 009/200: Loss=8.199879 (best so far)
(  11.125s) Epoch 010/200: Loss=8.195339 (best so far)
(  11.092s) Epoch 011/200: Loss=8.190530 (best so far)
(  11.090s) Epoch 012/200: Loss=8.185959 (best so far)
(  11.091s) Epoch 013/200: Loss=8.183935 (best so far)
(  11.189s) Epoch 014/200: Loss=8.179396 (best so far)
(  11.071s) Epoch 015/200: Loss=8.177390 (best so far)
(  11.089s) Epoch 016/200: Loss=8.174142 (best so far)
(  11.141s) Epoch 017/200: Loss=8.171781 (best so far)
(  11.151s) Epoch 018/200: Loss=8.169345 (best so far)
(  11.092s) Epoch 019/200: Loss=8.166943 (best so far)
(  11.085s) Epoch 020/200: Loss=8.165328 (best so far)
(  11.130s) Epoch 021/200: Loss=8.163620 (best so far)
(  11.163s) Epoch 022/200: Loss=8.162235 (best so far)
(  11.167s) Epoch 023/200: Loss=8.159946 (best so far)
(  11.157s) Epoch 024/200: Loss=8.159136 (best so far)
(  11.064s) Epoch 025/200: Loss=8.156610 (best so far)
(  11.153s) Epoch 026/200: Loss=8.155938 (best so far)
(  11.133s) Epoch 027/200: Loss=8.155148 (best so far)
(  11.107s) Epoch 028/200: Loss=8.153530 (best so far)
(  11.212s) Epoch 029/200: Loss=8.152866 (best so far)
(  11.170s) Epoch 030/200: Loss=8.150836 (best so far)
(  11.152s) Epoch 031/200: Loss=8.150010 (best so far)
(  11.077s) Epoch 032/200: Loss=8.148498 (best so far)
(  11.126s) Epoch 033/200: Loss=8.146802 (best so far)
(  11.146s) Epoch 034/200: Loss=8.146958
(  11.101s) Epoch 035/200: Loss=8.146459 (best so far)
(  11.083s) Epoch 036/200: Loss=8.145533 (best so far)
(  11.180s) Epoch 037/200: Loss=8.144811 (best so far)
(  11.190s) Epoch 038/200: Loss=8.143167 (best so far)
(  11.112s) Epoch 039/200: Loss=8.143225
(  11.075s) Epoch 040/200: Loss=8.142787 (best so far)
(  11.142s) Epoch 041/200: Loss=8.141464 (best so far)
(  11.206s) Epoch 042/200: Loss=8.141099 (best so far)
(  11.181s) Epoch 043/200: Loss=8.139977 (best so far)
(  11.212s) Epoch 044/200: Loss=8.139450 (best so far)
(  11.133s) Epoch 045/200: Loss=8.139698
(  11.143s) Epoch 046/200: Loss=8.139153 (best so far)
(  11.119s) Epoch 047/200: Loss=8.138890 (best so far)
(  11.201s) Epoch 048/200: Loss=8.137598 (best so far)
(  11.182s) Epoch 049/200: Loss=8.136797 (best so far)
(  11.181s) Epoch 050/200: Loss=8.136445 (best so far)
(  11.126s) Epoch 051/200: Loss=8.136379 (best so far)
(  11.142s) Epoch 052/200: Loss=8.135119 (best so far)
(  11.085s) Epoch 053/200: Loss=8.136219
(  11.149s) Epoch 054/200: Loss=8.136205
(  11.188s) Epoch 055/200: Loss=8.133819 (best so far)
(  11.181s) Epoch 056/200: Loss=8.134530
(  11.167s) Epoch 057/200: Loss=8.133956
(  11.158s) Epoch 058/200: Loss=8.133320 (best so far)
(  11.163s) Epoch 059/200: Loss=8.133205 (best so far)
(  11.137s) Epoch 060/200: Loss=8.132439 (best so far)
(  11.226s) Epoch 061/200: Loss=8.132223 (best so far)
(  11.167s) Epoch 062/200: Loss=8.132272
(  11.212s) Epoch 063/200: Loss=8.131921 (best so far)
(  11.189s) Epoch 064/200: Loss=8.131288 (best so far)
(  11.129s) Epoch 065/200: Loss=8.130972 (best so far)
(  11.117s) Epoch 066/200: Loss=8.131487
(  11.108s) Epoch 067/200: Loss=8.130274 (best so far)
(  11.111s) Epoch 068/200: Loss=8.130026 (best so far)
(  11.120s) Epoch 069/200: Loss=8.130421
(  11.188s) Epoch 070/200: Loss=8.129418 (best so far)
(  11.192s) Epoch 071/200: Loss=8.128736 (best so far)
(  11.182s) Epoch 072/200: Loss=8.128540 (best so far)
(  11.185s) Epoch 073/200: Loss=8.128376 (best so far)
(  11.105s) Epoch 074/200: Loss=8.128625
(  11.098s) Epoch 075/200: Loss=8.128163 (best so far)
(  11.090s) Epoch 076/200: Loss=8.127796 (best so far)
(  11.211s) Epoch 077/200: Loss=8.127523 (best so far)
(  11.225s) Epoch 078/200: Loss=8.126388 (best so far)
(  11.217s) Epoch 079/200: Loss=8.127579
(  11.108s) Epoch 080/200: Loss=8.127017
(  11.114s) Epoch 081/200: Loss=8.126539
(  11.118s) Epoch 082/200: Loss=8.126811
(  11.136s) Epoch 083/200: Loss=8.125866 (best so far)
(  11.130s) Epoch 084/200: Loss=8.126163
(  11.172s) Epoch 085/200: Loss=8.126152
(  11.159s) Epoch 086/200: Loss=8.125176 (best so far)
(  11.169s) Epoch 087/200: Loss=8.125053 (best so far)
(  11.164s) Epoch 088/200: Loss=8.125136
(  11.106s) Epoch 089/200: Loss=8.125202
(  11.155s) Epoch 090/200: Loss=8.124912 (best so far)
(  11.126s) Epoch 091/200: Loss=8.124737 (best so far)
(  11.132s) Epoch 092/200: Loss=8.123976 (best so far)
(  11.206s) Epoch 093/200: Loss=8.123848 (best so far)
(  11.157s) Epoch 094/200: Loss=8.124517
(  11.166s) Epoch 095/200: Loss=8.123458 (best so far)
(  11.196s) Epoch 096/200: Loss=8.123533
(  11.130s) Epoch 097/200: Loss=8.123479
(  11.150s) Epoch 098/200: Loss=8.122844 (best so far)
(  11.148s) Epoch 099/200: Loss=8.122321 (best so far)
(  11.121s) Epoch 100/200: Loss=8.122819
(  11.134s) Epoch 101/200: Loss=8.122500
(  11.151s) Epoch 102/200: Loss=8.122330
(  11.183s) Epoch 103/200: Loss=8.122073 (best so far)
(  11.155s) Epoch 104/200: Loss=8.122182
(  11.157s) Epoch 105/200: Loss=8.121909 (best so far)
(  11.153s) Epoch 106/200: Loss=8.122168
(  11.122s) Epoch 107/200: Loss=8.121382 (best so far)
(  11.171s) Epoch 108/200: Loss=8.121100 (best so far)
(  11.184s) Epoch 109/200: Loss=8.120713 (best so far)
(  11.222s) Epoch 110/200: Loss=8.121069
(  11.129s) Epoch 111/200: Loss=8.121035
(  11.212s) Epoch 112/200: Loss=8.120486 (best so far)
(  11.180s) Epoch 113/200: Loss=8.121362
(  11.188s) Epoch 114/200: Loss=8.120503
(  11.145s) Epoch 115/200: Loss=8.120468 (best so far)
(  11.098s) Epoch 116/200: Loss=8.120365 (best so far)
(  11.129s) Epoch 117/200: Loss=8.120167 (best so far)
(  11.115s) Epoch 118/200: Loss=8.119931 (best so far)
(  11.196s) Epoch 119/200: Loss=8.120060
(  11.151s) Epoch 120/200: Loss=8.119463 (best so far)
(  11.164s) Epoch 121/200: Loss=8.119569
(  11.157s) Epoch 122/200: Loss=8.118973 (best so far)
(  11.173s) Epoch 123/200: Loss=8.119049
(  11.144s) Epoch 124/200: Loss=8.119226
(  11.223s) Epoch 125/200: Loss=8.118988
(  11.231s) Epoch 126/200: Loss=8.119086
(  11.220s) Epoch 127/200: Loss=8.118808 (best so far)
(  11.191s) Epoch 128/200: Loss=8.118356 (best so far)
(  11.196s) Epoch 129/200: Loss=8.119051
(  11.165s) Epoch 130/200: Loss=8.118364
(  11.108s) Epoch 131/200: Loss=8.118832
(  11.154s) Epoch 132/200: Loss=8.118422
(  11.172s) Epoch 133/200: Loss=8.117776 (best so far)
(  11.102s) Epoch 134/200: Loss=8.118493
(  11.100s) Epoch 135/200: Loss=8.118654
(  11.154s) Epoch 136/200: Loss=8.118589
(  11.132s) Epoch 137/200: Loss=8.117760 (best so far)
(  11.150s) Epoch 138/200: Loss=8.118308
(  11.192s) Epoch 139/200: Loss=8.118029
(  11.211s) Epoch 140/200: Loss=8.117529 (best so far)
(  11.137s) Epoch 141/200: Loss=8.117301 (best so far)
(  11.100s) Epoch 142/200: Loss=8.117961
(  11.176s) Epoch 143/200: Loss=8.117297 (best so far)
(  11.224s) Epoch 144/200: Loss=8.117533
(  11.184s) Epoch 145/200: Loss=8.117699
(  11.181s) Epoch 146/200: Loss=8.117277 (best so far)
(  11.191s) Epoch 147/200: Loss=8.117311
(  11.176s) Epoch 148/200: Loss=8.117291
(  11.138s) Epoch 149/200: Loss=8.117270 (best so far)
(  11.122s) Epoch 150/200: Loss=8.116713 (best so far)
(  11.211s) Epoch 151/200: Loss=8.117322
(  11.101s) Epoch 152/200: Loss=8.117036
(  11.109s) Epoch 153/200: Loss=8.117284
(  11.147s) Epoch 154/200: Loss=8.117365
(  11.192s) Epoch 155/200: Loss=8.116705 (best so far)
(  11.201s) Epoch 156/200: Loss=8.116729
(  11.148s) Epoch 157/200: Loss=8.116484 (best so far)
(  11.167s) Epoch 158/200: Loss=8.116779
(  11.204s) Epoch 159/200: Loss=8.116746
(  11.180s) Epoch 160/200: Loss=8.116292 (best so far)
(  11.140s) Epoch 161/200: Loss=8.116614
(  11.136s) Epoch 162/200: Loss=8.116652
(  11.199s) Epoch 163/200: Loss=8.116278 (best so far)
(  11.219s) Epoch 164/200: Loss=8.116415
(  11.193s) Epoch 165/200: Loss=8.116531
(  11.053s) Epoch 166/200: Loss=8.116572
(  11.132s) Epoch 167/200: Loss=8.116693
(  11.210s) Epoch 168/200: Loss=8.115765 (best so far)
(  11.171s) Epoch 169/200: Loss=8.116410
(  11.134s) Epoch 170/200: Loss=8.116695
(  11.181s) Epoch 171/200: Loss=8.116495
(  11.173s) Epoch 172/200: Loss=8.116485
(  11.271s) Epoch 173/200: Loss=8.116012
(  11.191s) Epoch 174/200: Loss=8.116221
(  11.152s) Epoch 175/200: Loss=8.115842
(  11.227s) Epoch 176/200: Loss=8.115937
(  11.243s) Epoch 177/200: Loss=8.116570
(  11.201s) Epoch 178/200: Loss=8.116004
(  11.146s) Epoch 179/200: Loss=8.115616 (best so far)
(  11.211s) Epoch 180/200: Loss=8.116021
(  11.173s) Epoch 181/200: Loss=8.115832
(  11.226s) Epoch 182/200: Loss=8.115698
(  11.133s) Epoch 183/200: Loss=8.115981
(  11.135s) Epoch 184/200: Loss=8.115612 (best so far)
(  11.191s) Epoch 185/200: Loss=8.116192
(  11.167s) Epoch 186/200: Loss=8.115597 (best so far)
(  11.125s) Epoch 187/200: Loss=8.115778
(  11.140s) Epoch 188/200: Loss=8.116516
(  11.114s) Epoch 189/200: Loss=8.115865
(  11.133s) Epoch 190/200: Loss=8.115854
(  11.218s) Epoch 191/200: Loss=8.115987
(  11.213s) Epoch 192/200: Loss=8.115475 (best so far)
(  11.220s) Epoch 193/200: Loss=8.115941
(  11.198s) Epoch 194/200: Loss=8.115733
(  11.202s) Epoch 195/200: Loss=8.115880
(  11.212s) Epoch 196/200: Loss=8.115890
(  11.172s) Epoch 197/200: Loss=8.116339
(  11.162s) Epoch 198/200: Loss=8.115632
(  11.202s) Epoch 199/200: Loss=8.115748
(  11.131s) Epoch 200/200: Loss=8.115310 (best so far)
Training for 200 epochs took 2278.779s total and 11.394s average
Saving best model and options to exp10.pth
done!!!
