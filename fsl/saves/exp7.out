Namespace(T_max=20, backbone='resnet50', base_optimizer='SGD', batch_size=2048, dampening=0.0, dataset='cifar100fs', dataset_root=None, first_augment='CropResize', head='SimpleMLP', image_channels=3, image_mean=(0.5071, 0.4867, 0.4408), image_size=32, image_std=(0.2675, 0.2565, 0.2761), jitter_strength=1.0, loss_function='NTXent', model='SimCLRModel', momentum=0.05, nesterov=True, ntxent_temp=1.0, num_epochs=200, num_workers=4, pin_memory=True, pretrained=False, progress=False, projection_dim=128, save_path='exp7.pth', scheduler='CosineAnnealingLR', second_augment='GaussBlur', secondary_optimizer='LARS', shuffle=True, simple_opt=False, weight_decay=1e-06)
SimCLRModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (head): SimpleMLP(
    (l1): Linear(in_features=1000, out_features=512, bias=True)
    (relu): ReLU(inplace=True)
    (l2): Linear(in_features=512, out_features=128, bias=True)
  )
)
Starting Training
-----------------
(    30.145s) Epoch 001/200: Loss=7.929481 (best so far)
(    25.452s) Epoch 002/200: Loss=7.789076 (best so far)
(    25.161s) Epoch 003/200: Loss=7.691013 (best so far)
(    25.287s) Epoch 004/200: Loss=7.645053 (best so far)
(    25.323s) Epoch 005/200: Loss=7.620213 (best so far)
(    25.493s) Epoch 006/200: Loss=7.600826 (best so far)
(    25.580s) Epoch 007/200: Loss=7.587788 (best so far)
(    25.679s) Epoch 008/200: Loss=7.578648 (best so far)
(    25.650s) Epoch 009/200: Loss=7.571557 (best so far)
(    25.704s) Epoch 010/200: Loss=7.566940 (best so far)
(    25.668s) Epoch 011/200: Loss=7.563178 (best so far)
(    25.712s) Epoch 012/200: Loss=7.559953 (best so far)
(    25.709s) Epoch 013/200: Loss=7.557447 (best so far)
(    25.737s) Epoch 014/200: Loss=7.555337 (best so far)
(    25.680s) Epoch 015/200: Loss=7.554403 (best so far)
(    25.775s) Epoch 016/200: Loss=7.553240 (best so far)
(    25.733s) Epoch 017/200: Loss=7.552633 (best so far)
(    25.741s) Epoch 018/200: Loss=7.551937 (best so far)
(    25.756s) Epoch 019/200: Loss=7.551596 (best so far)
(    25.726s) Epoch 020/200: Loss=7.551713
(    25.768s) Epoch 021/200: Loss=7.551414 (best so far)
(    25.703s) Epoch 022/200: Loss=7.551555
(    25.753s) Epoch 023/200: Loss=7.552751
(    25.763s) Epoch 024/200: Loss=7.551545
(    25.902s) Epoch 025/200: Loss=7.551183 (best so far)
(    25.814s) Epoch 026/200: Loss=7.551287
(    25.684s) Epoch 027/200: Loss=7.551793
(    25.734s) Epoch 028/200: Loss=7.551011 (best so far)
(    25.710s) Epoch 029/200: Loss=7.551407
(    25.809s) Epoch 030/200: Loss=7.551622
(    25.794s) Epoch 031/200: Loss=7.551541
(    25.748s) Epoch 032/200: Loss=7.551381
(    25.784s) Epoch 033/200: Loss=7.551703
(    25.697s) Epoch 034/200: Loss=7.551227
(    25.922s) Epoch 035/200: Loss=7.551484
(    25.714s) Epoch 036/200: Loss=7.551063
(    25.756s) Epoch 037/200: Loss=7.550816 (best so far)
(    25.727s) Epoch 038/200: Loss=7.549264 (best so far)
(    25.810s) Epoch 039/200: Loss=7.548591 (best so far)
(    25.669s) Epoch 040/200: Loss=7.547015 (best so far)
(    25.729s) Epoch 041/200: Loss=7.545273 (best so far)
(    25.756s) Epoch 042/200: Loss=7.543889 (best so far)
(    25.811s) Epoch 043/200: Loss=7.542989 (best so far)
(    25.702s) Epoch 044/200: Loss=7.540448 (best so far)
(    25.801s) Epoch 045/200: Loss=7.539569 (best so far)
(    25.746s) Epoch 046/200: Loss=7.537039 (best so far)
(    25.792s) Epoch 047/200: Loss=7.536903 (best so far)
(    25.774s) Epoch 048/200: Loss=7.534655 (best so far)
(    25.818s) Epoch 049/200: Loss=7.533061 (best so far)
(    25.805s) Epoch 050/200: Loss=7.531418 (best so far)
(    25.745s) Epoch 051/200: Loss=7.531180 (best so far)
(    25.819s) Epoch 052/200: Loss=7.529725 (best so far)
(    25.775s) Epoch 053/200: Loss=7.529644 (best so far)
(    25.791s) Epoch 054/200: Loss=7.528687 (best so far)
(    25.721s) Epoch 055/200: Loss=7.527302 (best so far)
(    25.813s) Epoch 056/200: Loss=7.527144 (best so far)
(    25.781s) Epoch 057/200: Loss=7.526460 (best so far)
(    25.756s) Epoch 058/200: Loss=7.526237 (best so far)
(    25.746s) Epoch 059/200: Loss=7.526577
(    25.733s) Epoch 060/200: Loss=7.526463
(    25.803s) Epoch 061/200: Loss=7.526578
(    25.766s) Epoch 062/200: Loss=7.526764
(    25.795s) Epoch 063/200: Loss=7.526599
(    25.826s) Epoch 064/200: Loss=7.526876
(    25.785s) Epoch 065/200: Loss=7.526663
(    25.816s) Epoch 066/200: Loss=7.526755
(    25.807s) Epoch 067/200: Loss=7.526102 (best so far)
(    25.838s) Epoch 068/200: Loss=7.526192
(    25.910s) Epoch 069/200: Loss=7.527181
(    25.850s) Epoch 070/200: Loss=7.526692
(    25.760s) Epoch 071/200: Loss=7.526854
(    25.782s) Epoch 072/200: Loss=7.527560
(    25.821s) Epoch 073/200: Loss=7.527299
(    25.797s) Epoch 074/200: Loss=7.528432
(    25.812s) Epoch 075/200: Loss=7.529050
(    25.876s) Epoch 076/200: Loss=7.529131
(    25.750s) Epoch 077/200: Loss=7.528928
(    25.760s) Epoch 078/200: Loss=7.528270
(    25.754s) Epoch 079/200: Loss=7.529222
(    25.885s) Epoch 080/200: Loss=7.528511
(    25.783s) Epoch 081/200: Loss=7.527917
(    25.835s) Epoch 082/200: Loss=7.527848
(    25.733s) Epoch 083/200: Loss=7.526844
(    25.805s) Epoch 084/200: Loss=7.526240
(    25.771s) Epoch 085/200: Loss=7.525505 (best so far)
(    25.792s) Epoch 086/200: Loss=7.524233 (best so far)
(    25.794s) Epoch 087/200: Loss=7.523914 (best so far)
(    25.704s) Epoch 088/200: Loss=7.522772 (best so far)
(    25.819s) Epoch 089/200: Loss=7.521620 (best so far)
(    25.881s) Epoch 090/200: Loss=7.521443 (best so far)
(    25.753s) Epoch 091/200: Loss=7.520393 (best so far)
(    25.762s) Epoch 092/200: Loss=7.520032 (best so far)
(    25.819s) Epoch 093/200: Loss=7.518817 (best so far)
(    25.818s) Epoch 094/200: Loss=7.518704 (best so far)
(    25.770s) Epoch 095/200: Loss=7.517695 (best so far)
(    26.117s) Epoch 096/200: Loss=7.517994
(    25.804s) Epoch 097/200: Loss=7.518000
(    25.813s) Epoch 098/200: Loss=7.517761
(    25.726s) Epoch 099/200: Loss=7.517108 (best so far)
(    25.781s) Epoch 100/200: Loss=7.517415
(    25.851s) Epoch 101/200: Loss=7.517130
(    25.842s) Epoch 102/200: Loss=7.517502
(    25.740s) Epoch 103/200: Loss=7.516977 (best so far)
(    25.894s) Epoch 104/200: Loss=7.517095
(    25.847s) Epoch 105/200: Loss=7.517438
(    25.760s) Epoch 106/200: Loss=7.517278
(    25.785s) Epoch 107/200: Loss=7.517763
(    25.834s) Epoch 108/200: Loss=7.517290
(    25.856s) Epoch 109/200: Loss=7.517447
(    25.786s) Epoch 110/200: Loss=7.518142
(    25.797s) Epoch 111/200: Loss=7.518788
(    25.784s) Epoch 112/200: Loss=7.518349
(    25.828s) Epoch 113/200: Loss=7.519794
(    25.800s) Epoch 114/200: Loss=7.519089
(    25.819s) Epoch 115/200: Loss=7.520035
(    25.777s) Epoch 116/200: Loss=7.520243
(    25.863s) Epoch 117/200: Loss=7.521086
(    25.863s) Epoch 118/200: Loss=7.520209
(    25.806s) Epoch 119/200: Loss=7.520663
(    25.736s) Epoch 120/200: Loss=7.520476
(    25.850s) Epoch 121/200: Loss=7.520475
(    25.751s) Epoch 122/200: Loss=7.519738
(    25.767s) Epoch 123/200: Loss=7.519425
(    25.820s) Epoch 124/200: Loss=7.518557
(    25.846s) Epoch 125/200: Loss=7.518577
(    25.748s) Epoch 126/200: Loss=7.517404
(    25.819s) Epoch 127/200: Loss=7.516932 (best so far)
(    25.882s) Epoch 128/200: Loss=7.515716 (best so far)
(    25.719s) Epoch 129/200: Loss=7.516274
(    25.821s) Epoch 130/200: Loss=7.514874 (best so far)
(    25.719s) Epoch 131/200: Loss=7.514768 (best so far)
(    25.879s) Epoch 132/200: Loss=7.514643 (best so far)
(    25.731s) Epoch 133/200: Loss=7.512975 (best so far)
(    25.874s) Epoch 134/200: Loss=7.513454
(    25.743s) Epoch 135/200: Loss=7.513039
(    25.784s) Epoch 136/200: Loss=7.512770 (best so far)
(    25.890s) Epoch 137/200: Loss=7.512214 (best so far)
(    25.830s) Epoch 138/200: Loss=7.512816
(    25.797s) Epoch 139/200: Loss=7.512546
(    25.883s) Epoch 140/200: Loss=7.512512
(    25.772s) Epoch 141/200: Loss=7.511925 (best so far)
(    25.749s) Epoch 142/200: Loss=7.512547
(    25.805s) Epoch 143/200: Loss=7.511883 (best so far)
(    25.808s) Epoch 144/200: Loss=7.512471
(    25.803s) Epoch 145/200: Loss=7.512301
(    25.807s) Epoch 146/200: Loss=7.512376
(    25.824s) Epoch 147/200: Loss=7.512203
(    25.768s) Epoch 148/200: Loss=7.512728
(    25.827s) Epoch 149/200: Loss=7.513029
(    25.824s) Epoch 150/200: Loss=7.513494
(    25.705s) Epoch 151/200: Loss=7.513570
(    25.866s) Epoch 152/200: Loss=7.513794
(    25.826s) Epoch 153/200: Loss=7.513937
(    25.726s) Epoch 154/200: Loss=7.514155
(    25.731s) Epoch 155/200: Loss=7.514476
(    25.861s) Epoch 156/200: Loss=7.515491
(    25.786s) Epoch 157/200: Loss=7.515007
(    25.797s) Epoch 158/200: Loss=7.516134
(    25.933s) Epoch 159/200: Loss=7.516050
(    25.801s) Epoch 160/200: Loss=7.515571
(    25.757s) Epoch 161/200: Loss=7.515325
(    25.797s) Epoch 162/200: Loss=7.515917
(    25.785s) Epoch 163/200: Loss=7.514734
(    25.914s) Epoch 164/200: Loss=7.514807
(    25.823s) Epoch 165/200: Loss=7.514035
(    25.808s) Epoch 166/200: Loss=7.513450
(    25.816s) Epoch 167/200: Loss=7.513348
(    25.865s) Epoch 168/200: Loss=7.512461
(    25.800s) Epoch 169/200: Loss=7.511903
(    25.896s) Epoch 170/200: Loss=7.511951
(    25.802s) Epoch 171/200: Loss=7.512021
(    25.774s) Epoch 172/200: Loss=7.510918 (best so far)
(    25.775s) Epoch 173/200: Loss=7.509786 (best so far)
(    25.885s) Epoch 174/200: Loss=7.509600 (best so far)
(    25.846s) Epoch 175/200: Loss=7.509659
(    25.874s) Epoch 176/200: Loss=7.509271 (best so far)
(    25.852s) Epoch 177/200: Loss=7.509638
(    25.834s) Epoch 178/200: Loss=7.509061 (best so far)
(    25.816s) Epoch 179/200: Loss=7.508930 (best so far)
(    26.147s) Epoch 180/200: Loss=7.509939
(    25.854s) Epoch 181/200: Loss=7.508705 (best so far)
(    25.735s) Epoch 182/200: Loss=7.509054
(    25.840s) Epoch 183/200: Loss=7.508865
(    25.852s) Epoch 184/200: Loss=7.508892
(    25.699s) Epoch 185/200: Loss=7.509569
(    26.947s) Epoch 186/200: Loss=7.508994
(    25.755s) Epoch 187/200: Loss=7.509403
(    25.836s) Epoch 188/200: Loss=7.509325
(    25.725s) Epoch 189/200: Loss=7.509179
(    25.690s) Epoch 190/200: Loss=7.509568
(    25.755s) Epoch 191/200: Loss=7.510060
(    25.731s) Epoch 192/200: Loss=7.509829
(    25.694s) Epoch 193/200: Loss=7.510077
(    25.815s) Epoch 194/200: Loss=7.510988
(    25.744s) Epoch 195/200: Loss=7.511733
(    25.800s) Epoch 196/200: Loss=7.511863
(    25.736s) Epoch 197/200: Loss=7.512318
(    25.768s) Epoch 198/200: Loss=7.512337
(    25.797s) Epoch 199/200: Loss=7.512299
(    25.733s) Epoch 200/200: Loss=7.512050
Training for 200 epochs took 5161.472s total and 25.807s average
Saving model and options to exp7.pth
done!!!
