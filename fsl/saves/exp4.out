Namespace(T_max=20, backbone='resnet50', base_optimizer='SGD', batch_size=2048, dampening=0.0, dataset='cifar100fs', dataset_root=None, first_augment='CropResize', head='SimpleMLP', image_channels=3, image_mean=(0.5071, 0.4867, 0.4408), image_size=32, image_std=(0.2675, 0.2565, 0.2761), jitter_strength=1.0, loss_function='NTXent', model='SimCLRModel', momentum=0.1, nesterov=True, ntxent_temp=1.0, num_epochs=200, num_workers=4, pin_memory=True, pretrained=False, progress=False, projection_dim=128, save_path='exp4.pth', scheduler='CosineAnnealingLR', second_augment='GaussBlur', secondary_optimizer='LARS', shuffle=True, simple_opt=True, weight_decay=1e-06)
SimCLRModel(
  (backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (head): SimpleMLP(
    (l1): Linear(in_features=1000, out_features=512, bias=True)
    (relu): ReLU(inplace=True)
    (l2): Linear(in_features=512, out_features=128, bias=True)
  )
)
Starting Training
-----------------
(    22.455s) Epoch 001/200: Loss=8.429267 (best so far)
(    20.787s) Epoch 002/200: Loss=8.395215 (best so far)
(    20.403s) Epoch 003/200: Loss=7.965826 (best so far)
(    20.670s) Epoch 004/200: Loss=7.827199 (best so far)
(    20.865s) Epoch 005/200: Loss=7.775194 (best so far)
(    21.001s) Epoch 006/200: Loss=7.758933 (best so far)
(    21.021s) Epoch 007/200: Loss=7.748798 (best so far)
(    21.193s) Epoch 008/200: Loss=7.743345 (best so far)
(    20.959s) Epoch 009/200: Loss=7.731207 (best so far)
(    21.214s) Epoch 010/200: Loss=7.709938 (best so far)
(    21.215s) Epoch 011/200: Loss=7.696051 (best so far)
(    21.103s) Epoch 012/200: Loss=7.677771 (best so far)
(    21.129s) Epoch 013/200: Loss=7.669602 (best so far)
(    21.291s) Epoch 014/200: Loss=7.664152 (best so far)
(    21.333s) Epoch 015/200: Loss=7.662001 (best so far)
(    21.166s) Epoch 016/200: Loss=7.657071 (best so far)
(    21.342s) Epoch 017/200: Loss=7.656031 (best so far)
(    21.169s) Epoch 018/200: Loss=7.654138 (best so far)
(    21.334s) Epoch 019/200: Loss=7.649839 (best so far)
(    21.320s) Epoch 020/200: Loss=7.636391 (best so far)
(    21.240s) Epoch 021/200: Loss=7.633050 (best so far)
(    21.275s) Epoch 022/200: Loss=7.624179 (best so far)
(    21.179s) Epoch 023/200: Loss=7.622557 (best so far)
(    21.144s) Epoch 024/200: Loss=7.617826 (best so far)
(    21.401s) Epoch 025/200: Loss=7.615387 (best so far)
(    21.751s) Epoch 026/200: Loss=7.613470 (best so far)
(    21.380s) Epoch 027/200: Loss=7.613823
(    21.194s) Epoch 028/200: Loss=7.609145 (best so far)
(    21.395s) Epoch 029/200: Loss=7.606662 (best so far)
(    21.809s) Epoch 030/200: Loss=7.607480
(    21.231s) Epoch 031/200: Loss=7.607440
(    21.201s) Epoch 032/200: Loss=7.600333 (best so far)
(    21.221s) Epoch 033/200: Loss=7.599741 (best so far)
(    21.373s) Epoch 034/200: Loss=7.591983 (best so far)
(    21.394s) Epoch 035/200: Loss=7.587688 (best so far)
(    21.272s) Epoch 036/200: Loss=7.590389
(    21.260s) Epoch 037/200: Loss=7.584764 (best so far)
(    21.387s) Epoch 038/200: Loss=7.581708 (best so far)
(    21.394s) Epoch 039/200: Loss=7.580570 (best so far)
(    21.392s) Epoch 040/200: Loss=7.579889 (best so far)
(    21.249s) Epoch 041/200: Loss=7.578695 (best so far)
(    21.373s) Epoch 042/200: Loss=7.578569 (best so far)
(    21.159s) Epoch 043/200: Loss=7.577630 (best so far)
(    22.107s) Epoch 044/200: Loss=7.576415 (best so far)
(    21.571s) Epoch 045/200: Loss=7.573575 (best so far)
(    21.320s) Epoch 046/200: Loss=7.573181 (best so far)
(    21.355s) Epoch 047/200: Loss=7.574019
(    21.398s) Epoch 048/200: Loss=7.570224 (best so far)
(    21.663s) Epoch 049/200: Loss=7.570287
(    21.478s) Epoch 050/200: Loss=7.569875 (best so far)
(    21.527s) Epoch 051/200: Loss=7.569690 (best so far)
(    21.610s) Epoch 052/200: Loss=7.568332 (best so far)
(    21.405s) Epoch 053/200: Loss=7.570970
(    21.268s) Epoch 054/200: Loss=7.568654
(    21.285s) Epoch 055/200: Loss=7.568612
(    21.243s) Epoch 056/200: Loss=7.567816 (best so far)
(    21.312s) Epoch 057/200: Loss=7.566901 (best so far)
(    21.277s) Epoch 058/200: Loss=7.566499 (best so far)
(    21.273s) Epoch 059/200: Loss=7.567072
(    21.647s) Epoch 060/200: Loss=7.567145
(    21.314s) Epoch 061/200: Loss=7.566533
(    21.701s) Epoch 062/200: Loss=7.567474
(    21.336s) Epoch 063/200: Loss=7.566536
(    21.407s) Epoch 064/200: Loss=7.566360 (best so far)
(    21.333s) Epoch 065/200: Loss=7.565489 (best so far)
(    21.286s) Epoch 066/200: Loss=7.566368
(    21.348s) Epoch 067/200: Loss=7.565336 (best so far)
(    21.344s) Epoch 068/200: Loss=7.565309 (best so far)
(    21.623s) Epoch 069/200: Loss=7.565828
(    21.326s) Epoch 070/200: Loss=7.564988 (best so far)
(    21.333s) Epoch 071/200: Loss=7.565294
(    21.290s) Epoch 072/200: Loss=7.565088
(    21.367s) Epoch 073/200: Loss=7.563619 (best so far)
(    21.334s) Epoch 074/200: Loss=7.565082
(    21.317s) Epoch 075/200: Loss=7.565337
(    21.305s) Epoch 076/200: Loss=7.564273
(    21.543s) Epoch 077/200: Loss=7.564394
(    21.296s) Epoch 078/200: Loss=7.563403 (best so far)
(    21.326s) Epoch 079/200: Loss=7.563994
(    21.220s) Epoch 080/200: Loss=7.564255
(    21.294s) Epoch 081/200: Loss=7.563791
(    21.281s) Epoch 082/200: Loss=7.564122
(    21.329s) Epoch 083/200: Loss=7.562943 (best so far)
(    21.335s) Epoch 084/200: Loss=7.562688 (best so far)
(    21.308s) Epoch 085/200: Loss=7.562863
(    21.811s) Epoch 086/200: Loss=7.562244 (best so far)
(    21.329s) Epoch 087/200: Loss=7.562834
(    21.298s) Epoch 088/200: Loss=7.562275
(    21.570s) Epoch 089/200: Loss=7.562365
(    21.492s) Epoch 090/200: Loss=7.561175 (best so far)
(    21.482s) Epoch 091/200: Loss=7.560964 (best so far)
(    21.331s) Epoch 092/200: Loss=7.561778
(    21.267s) Epoch 093/200: Loss=7.560772 (best so far)
(    21.305s) Epoch 094/200: Loss=7.559868 (best so far)
(    21.266s) Epoch 095/200: Loss=7.560225
(    21.345s) Epoch 096/200: Loss=7.560692
(    21.447s) Epoch 097/200: Loss=7.560458
(    21.319s) Epoch 098/200: Loss=7.559503 (best so far)
(    21.319s) Epoch 099/200: Loss=7.558714 (best so far)
(    21.248s) Epoch 100/200: Loss=7.559209
(    21.813s) Epoch 101/200: Loss=7.557016 (best so far)
(    21.275s) Epoch 102/200: Loss=7.555768 (best so far)
(    21.325s) Epoch 103/200: Loss=7.556505
(    21.332s) Epoch 104/200: Loss=7.555986
(    21.588s) Epoch 105/200: Loss=7.556014
(    21.255s) Epoch 106/200: Loss=7.554592 (best so far)
(    21.328s) Epoch 107/200: Loss=7.554161 (best so far)
(    21.305s) Epoch 108/200: Loss=7.555338
(    21.376s) Epoch 109/200: Loss=7.551793 (best so far)
(    21.359s) Epoch 110/200: Loss=7.551450 (best so far)
(    21.259s) Epoch 111/200: Loss=7.548978 (best so far)
(    21.300s) Epoch 112/200: Loss=7.549512
(    21.305s) Epoch 113/200: Loss=7.549527
(    21.313s) Epoch 114/200: Loss=7.548206 (best so far)
(    21.638s) Epoch 115/200: Loss=7.547557 (best so far)
(    21.340s) Epoch 116/200: Loss=7.546205 (best so far)
(    21.865s) Epoch 117/200: Loss=7.546764
(    21.622s) Epoch 118/200: Loss=7.545099 (best so far)
(    21.268s) Epoch 119/200: Loss=7.544566 (best so far)
(    21.783s) Epoch 120/200: Loss=7.542962 (best so far)
(    21.297s) Epoch 121/200: Loss=7.544883
(    21.315s) Epoch 122/200: Loss=7.543431
(    21.306s) Epoch 123/200: Loss=7.542658 (best so far)
(    21.453s) Epoch 124/200: Loss=7.542827
(    21.492s) Epoch 125/200: Loss=7.540392 (best so far)
(    21.274s) Epoch 126/200: Loss=7.540138 (best so far)
(    21.260s) Epoch 127/200: Loss=7.541029
(    21.311s) Epoch 128/200: Loss=7.539431 (best so far)
(    21.302s) Epoch 129/200: Loss=7.538799 (best so far)
(    21.358s) Epoch 130/200: Loss=7.536844 (best so far)
(    21.299s) Epoch 131/200: Loss=7.538285
(    21.242s) Epoch 132/200: Loss=7.538273
(    21.278s) Epoch 133/200: Loss=7.536117 (best so far)
(    21.640s) Epoch 134/200: Loss=7.537420
(    21.250s) Epoch 135/200: Loss=7.535811 (best so far)
(    21.312s) Epoch 136/200: Loss=7.536186
(    21.380s) Epoch 137/200: Loss=7.534564 (best so far)
(    21.247s) Epoch 138/200: Loss=7.535074
(    21.310s) Epoch 139/200: Loss=7.534743
(    21.300s) Epoch 140/200: Loss=7.534719
(    21.289s) Epoch 141/200: Loss=7.532774 (best so far)
(    21.289s) Epoch 142/200: Loss=7.533312
(    21.328s) Epoch 143/200: Loss=7.531858 (best so far)
(    21.263s) Epoch 144/200: Loss=7.532594
(    21.311s) Epoch 145/200: Loss=7.531912
(    21.286s) Epoch 146/200: Loss=7.531100 (best so far)
(    21.299s) Epoch 147/200: Loss=7.531855
(    21.347s) Epoch 148/200: Loss=7.531415
(    21.399s) Epoch 149/200: Loss=7.531041 (best so far)
(    21.346s) Epoch 150/200: Loss=7.530805 (best so far)
(    21.290s) Epoch 151/200: Loss=7.530960
(    21.296s) Epoch 152/200: Loss=7.529936 (best so far)
(    21.293s) Epoch 153/200: Loss=7.529296 (best so far)
(    21.255s) Epoch 154/200: Loss=7.528457 (best so far)
(    21.673s) Epoch 155/200: Loss=7.528744
(    21.739s) Epoch 156/200: Loss=7.528526
(    21.296s) Epoch 157/200: Loss=7.527287 (best so far)
(    21.311s) Epoch 158/200: Loss=7.528768
(    21.281s) Epoch 159/200: Loss=7.527733
(    21.540s) Epoch 160/200: Loss=7.527791
(    21.400s) Epoch 161/200: Loss=7.526176 (best so far)
(    21.400s) Epoch 162/200: Loss=7.527547
(    21.290s) Epoch 163/200: Loss=7.525951 (best so far)
(    21.300s) Epoch 164/200: Loss=7.526283
(    21.321s) Epoch 165/200: Loss=7.525726 (best so far)
(    21.319s) Epoch 166/200: Loss=7.525431 (best so far)
(    21.325s) Epoch 167/200: Loss=7.525794
(    21.282s) Epoch 168/200: Loss=7.525565
(    21.473s) Epoch 169/200: Loss=7.525191 (best so far)
(    21.287s) Epoch 170/200: Loss=7.525514
(    21.333s) Epoch 171/200: Loss=7.526009
(    21.249s) Epoch 172/200: Loss=7.524578 (best so far)
(    21.303s) Epoch 173/200: Loss=7.523942 (best so far)
(    21.290s) Epoch 174/200: Loss=7.524099
(    21.355s) Epoch 175/200: Loss=7.524354
(    21.340s) Epoch 176/200: Loss=7.523364 (best so far)
(    21.335s) Epoch 177/200: Loss=7.524531
(    21.691s) Epoch 178/200: Loss=7.522779 (best so far)
(    21.311s) Epoch 179/200: Loss=7.522962
(    21.264s) Epoch 180/200: Loss=7.523829
(    21.323s) Epoch 181/200: Loss=7.522733 (best so far)
(    21.264s) Epoch 182/200: Loss=7.522734
(    21.278s) Epoch 183/200: Loss=7.522063 (best so far)
(    21.321s) Epoch 184/200: Loss=7.522244
(    21.280s) Epoch 185/200: Loss=7.522758
(    21.260s) Epoch 186/200: Loss=7.522216
(    21.371s) Epoch 187/200: Loss=7.522068
(    21.291s) Epoch 188/200: Loss=7.521657 (best so far)
(    21.339s) Epoch 189/200: Loss=7.521779
(    21.272s) Epoch 190/200: Loss=7.521585 (best so far)
(    21.348s) Epoch 191/200: Loss=7.521155 (best so far)
(    21.311s) Epoch 192/200: Loss=7.519875 (best so far)
(    21.299s) Epoch 193/200: Loss=7.520173
(    21.634s) Epoch 194/200: Loss=7.520526
(    21.280s) Epoch 195/200: Loss=7.520576
(    21.728s) Epoch 196/200: Loss=7.520566
(    21.369s) Epoch 197/200: Loss=7.520210
(    21.285s) Epoch 198/200: Loss=7.519337 (best so far)
(    21.364s) Epoch 199/200: Loss=7.520070
(    21.329s) Epoch 200/200: Loss=7.519796
Training for 200 epochs took 4269.332s total and 21.347s average
Saving model and options to exp4.pth
done!!!
